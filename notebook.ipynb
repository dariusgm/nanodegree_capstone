{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "## Capstone Project\n",
    "Darius Murawski\n",
    "12.09.2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Definition\n",
    "## Project Overview\n",
    "### Problem Domain\n",
    "\n",
    "Hard drives are used to save data from the operating system and different applications that are running on the server. The average price for a gigabyte is dropping and the demand for more space on the servers is growing. This results in a higher total number of hard drives running. As more drives are present in a data storage system, as more hard drives can fail, leading to data inconsistency and a major fail of the provided services. Since several years, hard drive vendors provide some values of this hard drives that reflect their current state. Based on this values, a broken drive can be identified. For more information on this so called S.M.A.R.T. values, see [wikipedia](https://en.wikipedia.org/wiki/S.M.A.R.T.). \n",
    "\n",
    "### Input Data overview\n",
    "\n",
    "A crash of a hard drive in a private environment is happening very rarely because the amount of total dives is very low. I searched and found a huge dataset provied by [backblaze.com](https://www.backblaze.com/b2/hard-drive-test-data.html). They are running a huge data storage system with several thousend hard drive for their customers. For each quarter, the show their running and failed drives in a csv format for further research. They have a licence for this data, that I like to cite at this place: \n",
    "\n",
    "`You can download and use this data for free for your own purpose, all we ask is three things 1) you cite Backblaze as the source if you use the data, 2) you accept that you are solely responsible for how you use the data, and 3) you do not sell this data to anyone, it is free.`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "Fails on a hard drive can be recovered using different techniques like raid settings or mirroring the data in different data centers. But the broken hard drive have to be replaced sooner or later with a new one, to make sure more fails on associated hard drives don’t break the data consistency. For each broken hard drive, somebody have to drive to the storage system, look it up in the storage system and replace it. This procedure have to be done each time one drive fails in the worst case and each time generating maintenance costs for the operating company. \n",
    "\n",
    "This costs can be reduced by replacing more drives than just the broken one by the maintenance people, as they only have to went to the storage system once and not several times. But what drives should they replace? In this Capstone project I want to generate a fail probability for each of the drives running in the storage system. The drives with a predicted fail should be replaced beforehand to reduce the maintenance costs in mid-range for the company operating the storage system.\n",
    "\n",
    "The described problem can be seen as a binary classification problem, where the allowed labels are only 0, for “hard drive is running” and 1 for “hard drive failed”. When generating predictions, 0 represents “hard drive will run further” and 1 for “hard drive will (soon) fail and should be replaced. \n",
    "\n",
    "### Related papers and articles\n",
    "Looking up in papers and articles, this kind of problem is referenced as predictive maintenance (PdM): Before waiting that something breaks, we replace the appropriate part in a regular maintenance to make sure that the entire system is able to continue running as expected.\n",
    "\n",
    "|Authors|Topic / Title|Document|\n",
    "|--|--|--|\n",
    "|Julia Scavicchio|Definition “Predictive Maintenance” (PdM)”|[Link](https://www.hippocmms.com/blog/3-cmms-trends-for-2016-millennials-mobility-and-machine-learning)\n",
    "|Jennifer Ho|Overview of industries, using Algorithms to reduce their machine downtime with further links|[Link](https://www.distrelec.de/current/en/artificial-intelligence/eliminating-machine-downtime-how-ai-is-transforming-maintenance/)\n",
    "|Taylor Short|What type of sensors can be used for predictive maintenance|[Link](https://www.softwareadvice.com/resources/predictive-maintenance-reduce-downtime/)\n",
    "|Gian Antonio Susto,<br> Andrea Schirru,<br> Simone Pampuri,<br> Seán McLoone,<br> Alessandro Beghi|Machine Learning for Predictive Maintenance: A Multiple Classifier Approach|[Link](https://ieeexplore.ieee.org/abstract/document/6879441)\n",
    "|Dr. Miguel A. Sanz Bobi,<br> Maria Cruz García,<br> Javier del Pico-Aznar | SIMAP: Intelligent System for Predictive Maintenance: Application to the health condition monitoring of a windturbine gearbox|[Link](https://www.sciencedirect.com/science/article/pii/S0166361506000534)\n",
    "|Hongfei Li,<br> Dhaivat Parikh,<br> Qing He,<br> Buyue Qian,<br> Zhiguo Li Dongping Fang,<br> Arun Hampapur| Improving rail network velocity: A machine learning approach to predictive maintenance | [Link](https://www.acsu.buffalo.edu/~qinghe/papers/journal/2014%20Railway%20Velocity.pdf)\n",
    "|Eduardo Pinheiro,<br> Wolf-Dietrich Weber,<br> Luiz Andre Barroso - Google| Failure Trends in a Large Disk Drive Population| [Link](http://static.googleusercontent.com/media/research.google.com/en/us/archive/disk_failures.pdf) or [Link](https://ai.google/research/pubs/pub32774)\n",
    "|Various|General Introduction into S.M.A.R.T.|[Link](https://en.wikipedia.org/wiki/S.M.A.R.T.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Statement\n",
    "\n",
    "I want to train a model that returns a \"1\", given by the provided features, that returns a prediction for a hard drive to fail. Drives with this value should be replaced by the maintenance team before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "The data is highly unbalanced. That results in using the F Beta Score to measure the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "  \n",
    "\n",
    "def metric(y_pred, y_true):\n",
    "    \"\"\"\n",
    "     Keyword arguments:\n",
    "     y_pred - a list or np.array object containing the predicted values from a machine learning algorithm.\n",
    "     y_true - a list or np.array object containing the correct labels that should match `y_pred`.\n",
    "     \n",
    "     As y_pred can have no predictions at all, we handle this special case and return a score of `0.0`.\n",
    "    \"\"\"\n",
    "    # Preventing calculation warning from fbeta_score\n",
    "    if np.array(y_pred).sum() == 0.0:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return fbeta_score(y_true, y_pred, average='macro', beta=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "For each day and each drive an entry is generated in a quarter file that is than later on compressed and made available to the public. Failed drives are also included in this dataset and on the next day removed from the list. The dataset contains of the following columns (see: [backblaze.com](https://www.backblaze.com/b2/hard-drive-test-data.html)):\n",
    "\n",
    "* Date – The date of the file in yyyy-mm-dd format.\n",
    "* Serial Number – The manufacturer-assigned serial number of the drive.\n",
    "* Model – The manufacturer-assigned model number of the drive.\n",
    "* Capacity – The drive capacity in bytes.\n",
    "* Failure – Contains a “0” if the drive is OK. Contains a “1” if this is the last day the drive was operational before failing.\n",
    "* Normalized and Raw S.M.A.R.T. values from 1 upto 255. The data have a different set of S.M.A.R.T. values.\n",
    "\n",
    "The normalized values are sometimes not provided, for examle to return the amount of hours a drive was already running a normalization makes no sence. I decided to only use the raw values. The Ranges of the values are vendor specific. Thats why I decided not to build a model for everything, but instead generate a model specific one.\n",
    "\n",
    "The entire dataset (I call it `raw`) is split into several pieces. Each piece referencing the year and the quarter that this data was extracted from. The following table shows some more detailed information about the dataset:\n",
    "\n",
    "|file|year|quarter(s)|compressed MB|uncompressed MB|files|\n",
    "|--  |  --|       --|           --|             --|   --|\n",
    "|[data_2013.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2013.zip)|2013|Q1,Q2,Q3,Q4|77|738|266|\n",
    "|[data_2014.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2014.zip)|2014|Q1,Q2,Q3,Q4|560|2880|365|\n",
    "|[data_2015.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_2015.zip)|2015|Q1,Q2,Q3,Q4|803|4294|366|\n",
    "|[data_Q1_2016.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2016.zip)|2016|Q1|257|1356|92|\n",
    "|[data_Q2_2016.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2016.zip)|2016|Q2|278|1478|92|\n",
    "|[data_Q3_2016.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2016.zip)|2016|Q3|307|1604|92|\n",
    "|[data_Q4_2016.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2016.zip)|2016|Q4|321|1651|92|\n",
    "|[data_Q1_2017.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2017.zip)|2017|Q1|323|1659|90|\n",
    "|[data_Q2_2017.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q2_2017.zip)|2017|Q2|368|1895|91|\n",
    "|[data_Q3_2017.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q3_2017.zip)|2017|Q3|406|2027|92|\n",
    "|[data_Q4_2017.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q4_2017.zip)|2017|Q4|434|2112|93|\n",
    "|[data_Q1_2018.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2018.zip)|2018|Q1|484|2381|90|\n",
    "|[data_Q1_2018.zip](https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data/data_Q1_2018.zip)|2018|Q2|502|2472|91|\n",
    "|Total|2013 - 2018|14|5128|26536|1912|\n",
    "\n",
    "For the first years, the data was collected on a year basis, but then starting from 2016, the data was splitted by quarter.\n",
    "\n",
    "The Input Features are the hard drive model, and the returned S.M.A.R.T. values of the drive at the timestamp represented by \"date\". The data is highly unbalanced, as only about 1.8% drives in the reporting period between April 2003 and June 2018 failed (see: [backblaze.com](https://www.backblaze.com/blog/hard-drive-stats-for-q2-2018/)) .\n",
    "\n",
    "As of time writing, Q2 for 2018 was the latest dataset. Note that the amount of information changed over time. From 2013 to 2014, 80 columns of data were collected for each drive. From 2015 to 2017 90 columns of data were collected. For Q2 2018, 104 columns with data was collected. Each reflecting a subset of the possible 256 S.M.A.R.T. \"columns\" with the raw and the normalized value. \n",
    "\n",
    "I choosed as an example the hard drive model \"ST6000DX000\" by Seagate for the visualization part. Examples are provided in \"Exploratory Visualisation\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mapping and the preferred value-range of each S.M.A.R.T. value was extracted from [wikipedia.org](https://en.wikipedia.org/wiki/S.M.A.R.T.) - I extracted the map to the [helper.py](helper.py) to have a better overview inside the nodebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could extract three groups of features based on the following plots:\n",
    "* Feature that seem to not be correlated to a fail of the given hard drive model\n",
    "* Feature that are supposed to have a correlation, proposed by wikipedia\n",
    "* Feature that seems to have a correlation, based on the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import Helper\n",
    "helper = Helper()\n",
    "smart_to_name = helper.smart_to_name()\n",
    "\n",
    "# We also need to define the basic column names, that are also present beside the S.M.A.R.T. values. \n",
    "# They are also extracted to the helper.py\n",
    "column_list = helper.column_list()\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import random\n",
    "import glob\n",
    "import sys\n",
    "import requests\n",
    "import shutil\n",
    "import zipfile\n",
    "# see: https://docs.python.org/3/library/concurrent.futures.html\n",
    "from concurrent.futures import Executor, ThreadPoolExecutor\n",
    "import math\n",
    "from math import floor\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from time import time, sleep\n",
    "\n",
    "from traceback import print_stack\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model.stochastic_gradient import SGDClassifier\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble.weight_boosting import AdaBoostClassifier\n",
    "from sklearn.ensemble.forest import RandomForestClassifier\n",
    "from sklearn.neural_network.multilayer_perceptron import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning:\n",
      "\n",
      "Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# For clean resetting: https://stackoverflow.com/questions/45063602/attempting-to-reset-tensorflow-graph-when-using-keras-failing\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sure that my C: drives is not full of the downloaded data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('D:','capstone')):\n",
    "    os.chdir(os.path.join('D:','capstone'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(os.path.join('drives', 'ST6000DX000.csv')):\n",
    "    raise Exception('Please run first the data preprocessing steps, before rerunning this cells as they depend on the preprocessing results!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df = pd.read_csv(os.path.join('drives', 'ST6000DX000.csv'), names=column_list, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df.dropna(inplace=True, how='all', axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "st_df.rename(columns=smart_to_name, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets take a look at the mapped data. Missing columns, that were generated while preprocessing, are now removed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>model</th>\n",
       "      <th>capacity_bytes</th>\n",
       "      <th>failure</th>\n",
       "      <th>Read Error Rate</th>\n",
       "      <th>Spin-Up Time</th>\n",
       "      <th>Start/Stop Count</th>\n",
       "      <th>Reallocated Sectors Count</th>\n",
       "      <th>Seek Error Rate</th>\n",
       "      <th>...</th>\n",
       "      <th>Power-off Retract Count</th>\n",
       "      <th>Load Cycle Count</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Hardware ECC Recovered</th>\n",
       "      <th>Current Pending Sector Count</th>\n",
       "      <th>(Offline) Uncorrectable Sector Count</th>\n",
       "      <th>UltraDMA CRC Error Count</th>\n",
       "      <th>Head Flying Hours</th>\n",
       "      <th>Total LBAs Written</th>\n",
       "      <th>Total LBAs Read</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Z4D05G2K</td>\n",
       "      <td>ST6000DX000</td>\n",
       "      <td>6001175126016</td>\n",
       "      <td>0</td>\n",
       "      <td>15044280</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1145162644</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>219262</td>\n",
       "      <td>25</td>\n",
       "      <td>15044280</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>146690313049158</td>\n",
       "      <td>38236743440</td>\n",
       "      <td>81807331858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Z4D0ARTZ</td>\n",
       "      <td>ST6000DX000</td>\n",
       "      <td>6001175126016</td>\n",
       "      <td>0</td>\n",
       "      <td>12179803</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1300171494</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>78657</td>\n",
       "      <td>24</td>\n",
       "      <td>12179803</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215061897435155</td>\n",
       "      <td>46615460952</td>\n",
       "      <td>47540610070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Z4D07B6E</td>\n",
       "      <td>ST6000DX000</td>\n",
       "      <td>6001175126016</td>\n",
       "      <td>0</td>\n",
       "      <td>135238309</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1405133808</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1447</td>\n",
       "      <td>28</td>\n",
       "      <td>135238309</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207056078396721</td>\n",
       "      <td>29226952206</td>\n",
       "      <td>134425865010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Z4D069T5</td>\n",
       "      <td>ST6000DX000</td>\n",
       "      <td>6001175126016</td>\n",
       "      <td>0</td>\n",
       "      <td>198113002</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1313851777</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>33635</td>\n",
       "      <td>32</td>\n",
       "      <td>198113002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79276506372450</td>\n",
       "      <td>27643151347</td>\n",
       "      <td>175181733315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-11-10</td>\n",
       "      <td>Z4D03TNS</td>\n",
       "      <td>ST6000DX000</td>\n",
       "      <td>6001175126016</td>\n",
       "      <td>0</td>\n",
       "      <td>180317125</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5644114135</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>68349</td>\n",
       "      <td>21</td>\n",
       "      <td>180317125</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142073223206904</td>\n",
       "      <td>38344122896</td>\n",
       "      <td>94875351761</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         date serial_number        model  capacity_bytes  failure  \\\n",
       "0  2017-11-10      Z4D05G2K  ST6000DX000   6001175126016        0   \n",
       "1  2017-11-10      Z4D0ARTZ  ST6000DX000   6001175126016        0   \n",
       "2  2017-11-10      Z4D07B6E  ST6000DX000   6001175126016        0   \n",
       "3  2017-11-10      Z4D069T5  ST6000DX000   6001175126016        0   \n",
       "4  2017-11-10      Z4D03TNS  ST6000DX000   6001175126016        0   \n",
       "\n",
       "   Read Error Rate  Spin-Up Time  Start/Stop Count  Reallocated Sectors Count  \\\n",
       "0         15044280             0                11                          0   \n",
       "1         12179803             0                 5                          0   \n",
       "2        135238309             0                15                          0   \n",
       "3        198113002             0                12                          0   \n",
       "4        180317125             0                15                          0   \n",
       "\n",
       "   Seek Error Rate       ...         Power-off Retract Count  \\\n",
       "0       1145162644       ...                               0   \n",
       "1       1300171494       ...                               0   \n",
       "2       1405133808       ...                               0   \n",
       "3       1313851777       ...                               0   \n",
       "4       5644114135       ...                               0   \n",
       "\n",
       "   Load Cycle Count  Temperature  Hardware ECC Recovered  \\\n",
       "0            219262           25                15044280   \n",
       "1             78657           24                12179803   \n",
       "2              1447           28               135238309   \n",
       "3             33635           32               198113002   \n",
       "4             68349           21               180317125   \n",
       "\n",
       "   Current Pending Sector Count  (Offline) Uncorrectable Sector Count  \\\n",
       "0                             0                                     0   \n",
       "1                             0                                     0   \n",
       "2                             0                                     0   \n",
       "3                             0                                     0   \n",
       "4                             0                                     0   \n",
       "\n",
       "   UltraDMA CRC Error Count  Head Flying Hours  Total LBAs Written  \\\n",
       "0                         2    146690313049158         38236743440   \n",
       "1                         0    215061897435155         46615460952   \n",
       "2                         0    207056078396721         29226952206   \n",
       "3                         0     79276506372450         27643151347   \n",
       "4                         0    142073223206904         38344122896   \n",
       "\n",
       "   Total LBAs Read  \n",
       "0      81807331858  \n",
       "1      47540610070  \n",
       "2     134425865010  \n",
       "3     175181733315  \n",
       "4      94875351761  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if any unmapped columns exist, that we have to take a look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', 'serial_number', 'model', 'capacity_bytes', 'failure',\n",
       "       'Read Error Rate', 'Spin-Up Time', 'Start/Stop Count',\n",
       "       'Reallocated Sectors Count', 'Seek Error Rate', 'Power-On Hours',\n",
       "       'Spin Retry Count', 'Power Cycle Count', 'SATA Downshift Error Count',\n",
       "       'End-to-End error', 'Reported Uncorrectable Errors', 'Command Timeout',\n",
       "       'High Fly Writes', 'Temperature Difference', 'G-sense Error Rate',\n",
       "       'Power-off Retract Count', 'Load Cycle Count', 'Temperature',\n",
       "       'Hardware ECC Recovered', 'Current Pending Sector Count',\n",
       "       '(Offline) Uncorrectable Sector Count', 'UltraDMA CRC Error Count',\n",
       "       'Head Flying Hours', 'Total LBAs Written', 'Total LBAs Read'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no, everything is mapped given a name. Now lets define the plotting method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature(feature, df):\n",
    "    plt.boxplot(\n",
    "            [np.array(df[df['failure'] == 0][feature]), np.array(df[df['failure'] == 1][feature])],            \n",
    "            vert=False,\n",
    "            labels=['running: {}'.format(feature),'failed: {}'.format(feature)],\n",
    "            autorange=True, widths=0.9)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not Correlated Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('Read Error Rate', st_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot is an example to have no visual correlation between the failing of a drive and this S.M.A.R.T. value. It is not useful in our machine learning context. For most of the features, this is the case.\n",
    "\n",
    "* Preferred: Low Values\n",
    "* Not normalized between different hard drive vendor\n",
    "* Correlation to Fail by Wikipedia: No"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAecAAAD8CAYAAABNa2y4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAEXBJREFUeJzt3XuQZGV9xvHvAwsBlItxkfLCZtUISlmCMgqJQJaEJIplECEFgcSAWhZlBSLGWwVL1xATI1YlUUoJReEGCy8VBLNeUFFEQC7LLizLzVUCAREUBURFxSz88kef1Xac3ulZunvecb+fqlN1zun3fc/v7d7qZ87bPbOpKiRJUju2mu8CJEnSrzKcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY1ZNN8FaGFavHhxLV26dL7LkKQFZc2aNd+vql1na2c4a7MsXbqU1atXz3cZkrSgJLljmHYua0uS1BjDWZKkxhjOkiQ1xnCWJKkxhrMmb/nO812BJDXNcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJM1BkrFfw3CWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjRhrOSU5KckuSczfRZirJ+7v945KcPsdr/G+SxbO0WZHk9iRru+2KOV5jeZI3zXB+lySvn8tYfeN9u6+etUl2mUP/OT9PkqSFa9GIx3s9cEhV3TWoQVWtBlaP+LozeXNVnTfiMXehN8cPbkbff62q9424nlklWVRVGwYdb6JfgFTVo2MtUJL0a0Z255zkDOAZwIVJTk7yoiRXJrkuyRVJ9uzaLUvymRn675rkk0mu6bYXd+efmOSLSW5KchaQx1Dj8iRnJ7kkyW1JTup77JQk30hyObDngCHeAzyzu/M9LT2nJbkxyQ1JjppjPcclOT/J55N8M8l7+x47vqtnFfDiAf0f181nVfc8H9Y37sokFwNf7p7zy5KsBG7u2ryxq/vGJG/ozi1Nsj7JOcCNwO5zmY8kaTRGdudcVSckeQlwcFV9P8lOwIFVtSHJIcA/AUdsYoh/p3d3eXmSJcAXgOcA7wQur6p/SPIy4DUbOyT5HPDaqrp7hvFOS/L2bv+mqjq22382cDCwI7A+yYeA5wFHA/vQe06uBdbMMObbgOdW1T7d9Y/o+uwNLAauSXJpVd0zQ9+Tk/xlt/9AVR3c7e8DPB94uKvnA8AG4F3AvsCDwFeA62YY8xTg4qp6dbdMvirJl7rHXgA8r6ruT7KsO35uVd2eZF/geGA/ej/sXJ3kq8ADwLOAv66qq2a4niRpAka9rN1vZ+A/kzwLKGCbWdofAuzVW00FYKckjwcOAl4JUFWfTfLAxgZVdegmxhu0rP3ZqnoYeDjJvcBuwIHABVX1E4DuDnMYBwAfq6pHgO92AfdCYKb+g5a1v1xVD3bXvRn4HXpBf0lVfa87/wlgjxn6/gnwZ32fj28HLOn2L6qq+/varqqq2/vqvqCqHurGP5/ec7ASuGNQMCd5HfA6gCVLlszUZGh9r7MkaZpxhvOpwFeq6vAkS4FLZmm/FbB/Vf2s/+QY3sQf7tt/hE08B0l2Bz7dHZ4BfH6YCyR5N/AygI132aOoZ6ZLAUdU1fpp198PeGha2+nHgwxsV1VnAmcCTE1N1RzqnGmsx9JdkubNJG4uxvmrVDsD3+72jxui/ReBEzceJNkYapcCx3TnXgo8YXQl/sKlwCuSbJ9kR+DlAFX1rarap9vOAH5Ebzl8o8uAo5JsnWRXenf5q6rqlI39NrOeq4E/6D5v3wb48wHtvgCc2H15iyTPH3L8y+jNd4ckjwMO785JkhowznB+L/DPSa5juLvBk4CpJOu65d0TuvPvAg5KchO95e07N3ZI8rkkTxkw3mn51V9d2nbQhavqWuATwPXAhcA1A9rdB3yt+xLVacAFwLqu38XAW6rqOwMuc/K0epZuop57gOXAlcDXgFsGND2V3scF67rn59RBY04b/1pgBbCK3g8CZ1XVTJ9pS5LmQVxe1OaYmpqq1as38zfilu8Myx8cbUGSNCFJNvujuSRrqmpqtnb+hTBJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkaQ6qauzXMJwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZw1ecsfnO8KJKlphrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZw1ect3nu8KJKlphrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJGkOkoz9GoazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaM6/hnOSsJHuNYdxLkqxPsrbbzptj/xVJjpzh/NIkx2xGPSuS3N5XzxVz7L88yZvmel1J0sK0aNiGSQKkqh4d1cWr6rWjGmsGx1bV6hGPuRQ4BvjoZvR9c1XN6YeEUUiyqKo2DDoetp8kaXI2eefc3SmuT3IOcCOwe5If9z1+ZJIV3f6KJO9PckWS2zbeeSZZ1t3Jnpfk60nO7YJ+4x3uVLf/4yTvTnJ9kquS7Nadf2Z3fEOSf+y//lxtosYkOb2b65eAJw0Y4j3Agd3d78lJtkvy4a6265IcPMd6lic5u3sebktyUt9jpyT5RpLLgT0H9N81ySeTXNNtL+4b9yNJvgZ8JMlxSVYmuRj4cjff05Lc2NV+VNdvWZLLkqwEbp7LXCRJI1RVAzd6d4qPAvv3nftx3/6RwIpufwXwX/QCfy/g1u78MuBB4GndY1cCB3SPXQJMdfsFvLzbfy/w9m7/M8BfdPsnTLv+2gF1XwKsB9Z222mz1PhK4CJga+ApwA+AI2cYdxnwmb7jvwPO7vafDdwJbDdDvxXA7X31nNudXw5cAfwWsBi4D9gG2Be4AdgB2Am4FXjTDON+tO+5XALc0jfuGmD77vg44C7gt7vjI/rmu1tX95O7+T0EPH3A8/o6YDWwesmSJbXZ3rlTda+3m5ub24LcNheweqb31+nbMMvad1TVVUO0A/hU9Za9b95459tZVVV3ASRZSy/0L5/W9+f0ghh6wfLH3f7vAa/o9j8KvG9jh6raZxO1DFrWnqnGg4CPVdUjwN3dHeYwDgA+0NXy9SR3AHsA62ZoO2hZ+7NV9TDwcJJ76YXlgcAFVfUTgO5OdiaHAHt1CxEAOyV5fLe/sqp+2tf2oqq6v6/ujfP9bpKvAi8Efkjvtbp9potV1ZnAmQBTU1M1oKah9P6NStLC0/eeOzbDhPND047731W3m/bYw337GXD+kQHX/b/65Tv2oDajMKjGX5NkP+A/usN30AuvWSX5MPB84O6qOnQO9cx13lvRW9X42bTrw6+/btOPBxm2nSRpTDbn29rfTfKcJFsBh4+6oBlcRW8ZFuDoMV3jUuCoJFsneTJwMEBVXV1V+3TbSuBHwI59/S4DjgVIsge9peX1VXV812e2YN5UPa9Isn2SHYGXD2j3ReDEjQdJNrWS0O8yfjnfXemtHKzazFolSSO2OeH8NnrLz1cA94y2nBm9AXhjknXA79L7/Br4xRL5IOf2/erSl2a5xgXAN+l9Ceocep+Lz2Qd8Ej3pbWTgQ8CWyW5AfgEcFy3RD2T0/rqWZtk20HFVNW13XjXAxcC1wxoehIwlWRdkpvpfSY/jAu6uVwPXAy8paq+M2RfSdKYpfXP/pLsAPy0qirJ0fS+HHbYfNe1pZuamqrVqzfzN9WW7wzLH5y9nSQ1KMlmf28myZqqmpqt3bg+1x2lfYHTu1+/+gHw6nmuR5KksWo+nKvqMmDv+a5DkqRJ8W9rS5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnCWJKkxhrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkzUFVjf0ahrMkSY0xnCVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmNMZwlSWqM4SxJUmMMZ0mSGmM4S5LUGMNZkqTGGM6SJDXGcJYkqTGGsyRJjTGcJUlqjOEsSVJjDGdJkhpjOEuS1BjDWZKkxhjOkiQ1xnDW5C1/cL4rkKSmGc6SJDXGcJYkqTGGsyRJjTGcJUlqTKpqvmvQApTke8Adm9l9MfD9EZazEDjnLYNz3jI8ljn/TlXtOlsjw1kTl2R1VU3Ndx2T5Jy3DM55yzCJObusLUlSYwxnSZIaYzhrPpw53wXMA+e8ZXDOW4axz9nPnCVJaox3zpIkNcZw1tgkeUmS9UluTfK2GR5Pkvd3j69L8oL5qHOUhpjzsd1cb0hyRZK956POUZptzn3tXphkQ5IjJ1nfOAwz5yTLkqxNclOSr066xlEb4t/2zkk+neT6bs7Hz0edo5Lk7CT3JrlxwOPjff+qKje3kW/A1sD/AM8AtgWuB/aa1uZQ4EIgwP7A1fNd9wTm/PvAE7r9l24Jc+5rdzHwOeDI+a57Aq/zLsDNwJLu+EnzXfcE5vz3wL90+7sC9wPbznftj2HOBwEvAG4c8PhY37+8c9a4vAi4tapuq6qfAx8HDpvW5jDgnOq5CtglyZMnXegIzTrnqrqiqh7oDq8CnjbhGkdtmNcZ4ETgk8C9kyxuTIaZ8zHA+VV1J0BVLfR5DzPnAnZMEuDx9MJ5w2TLHJ2qupTeHAYZ6/uX4axxeSrwrb7ju7pzc22zkMx1Pq+h95P3QjbrnJM8FTgc+NAE6xqnYV7nPYAnJLkkyZokr5pYdeMxzJxPB54D3A3cAPxtVT06mfLmxVjfvxaNaiBJw0tyML1wPmC+a5mAfwPeWlWP9m6qtgiLgH2BPwK2B65MclVVfWN+yxqrPwXWAn8IPBO4KMllVfXD+S1rYTKcNS7fBnbvO35ad26ubRaSoeaT5HnAWcBLq+q+CdU2LsPMeQr4eBfMi4FDk2yoqk9NpsSRG2bOdwH3VdVDwENJLgX2BhZqOA8z5+OB91TvA9lbk9wOPBtYNZkSJ26s718ua2tcrgGeleTpSbYFjgZWTmuzEnhV963H/YEHq+qeSRc6QrPOOckS4Hzgr35D7qJmnXNVPb2qllbVUuA84PULOJhhuH/b/w0ckGRRkh2A/YBbJlznKA0z5zvprRSQZDdgT+C2iVY5WWN9//LOWWNRVRuS/A3wBXrf9Dy7qm5KckL3+Bn0vrl7KHAr8BN6P3kvWEPO+R3AE4EPdneSG2oB/6cBQ875N8owc66qW5J8HlgHPAqcVVUz/krOQjDk63wqsCLJDfS+wfzWqlqw/1tVko8By4DFSe4C3glsA5N5//IvhEmS1BiXtSVJaozhLElSYwxnSZIaYzhLktQYw1mSpMYYzpIkNcZwliSpMYazJEmN+X8RSQ14JbF3awAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e2afb21d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('End-to-End error', st_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the feature `End-to-End error`, we have the same boxplot for failed and for running devices. This value should be the count of parity errors. Its also interisting that this value is normalized from 0 upto 1. The representation as \"error count\" is not obvious at this point from a user perspective. From this representation I can't see an possible correlation with a failing drive. \n",
    "\n",
    "* Preferred: Low\n",
    "* Correlation to Fail by Wikipedia: Yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e70dd3c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('Reported Uncorrectable Errors', st_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the interisting features was `Reported Uncorrectable Errors` - this indeed looks very promising to be correlated to a hard drive fail, as the range of fail hard drives jumps upto 12 instead of being lower than 2.\n",
    "\n",
    "* Referred: Low\n",
    "* Correlation to Fail by Wikipedia: Yes\n",
    "* **Correlation to Fail by given plot: Yes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAioAAAD8CAYAAABZyI+oAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAFsdJREFUeJzt3X2wZVV95vHvw4uCvJkEJGhk2hfQNAot3ZCIiDBDSDQaYCQDSCVSGt/CoOjIFHFSsZ3EiCEZC0RUJApaoKARB40KqCBEINLNS0PzokbAoIhQiQKGAcXf/LHXtU9f7u17u7ndZ9H3+6nq6n322Xuv3157V5/nrL0OpKqQJEnq0SbjLkCSJGk6BhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVubjbsA6fFu++23rwULFoy7DEl63Fi+fPm9VbXDbLY1qEiP0YIFC1i2bNm4y5Ckx40kd8x2Wx/9SJKkbhlUJElStwwqkiSpWwYVSZLULYOKNE5Ltxt3BZLUNYOKJEnqlkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZK01pJskHYMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKJEnqlkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHVrToNKkjcnuTnJ2WvYZkmSU9ry0UlOXcs2bk+y/Sy2e3uSW5Jcl+TqJH+8Nu3MhXZ+T53mvTOT3NbquybJC+eozTOTHNaWz0iycI6O+7+SrEyyotX8W+twjAVJXjUHtfx6kk8l+Zcky5N8Mcmuj/W4k9rYP8k+c3lMSdLa22yOj/enwIFVded0G1TVMmDZHLe7miRvBH4H2Luq7kuyLXDoWuy/WVX9fLrXa+Fo4EbgB9O8f3xVfSbJQcCHgd3XoY1pVdWfzMVxWoh6ObBnVT3UguIT1uFQC4BXAeesRduTr0WA84GzquqItm4PYEfgW+tQ03T2Bx4ArpjDY0qS1tKcjagk+RDwTOBLSd6aZO8kVya5NskVSZ7Ttts/yRem2H+HJP/QRj+uTvKitv7XklzUvs2fAWQW5bwDeFNV3QdQVfdV1VnteL8ckWmjO5e25aVJPpHkG8An2mjIBUm+Bny1bXN8q21Fkne1dQvaKNJHWo0XJdmyjWosAc5uIxBbrqHey4Bnt+M9K8mX20jB5Ume29afmeSU1pffHRk1SZJTk9ya5CvAU0b69NIkS9ryA0neneT6JFcl2XGkvauS3JDkr5I8MEV9OwH3VtVDrT/vraoftP0XJ/l6q/fCJDu19c9O8pXW3jVJngWcCLy49cdbk2yR5GOt7WuTHND2fVTfjzgA+FlVfWhiRVVdX1WXt744KcmN7ZiHt+Otds+1/jp65H54V6vxhiTPTbIAeCPw1lbri9dw7SRJ69GcBZWqeiPDyMEBVfU+4BbgxVX1AuAvgL+e4RAnA++rqr2AVwJntPXvBP6pqnZj+Ca988QObch/tUcrbfRkm6r67jqcxkKGEaEj2+s9gcOq6iVt1GMXYG9gEbA4yX5tu12AD7Qafwy8sqo+wzBydFRVLaqqB9fQ7iuAG9ry6cCxVbUYeDtw2sh2OwH7MoxunNjWHQo8p9X+x8B0jyu2Aq6qqj0YgtHr2vqTgZOr6vnAdCNhFwFPT/KtJKcleQlAks2B97c+Wgx8FHh32+fs1id7tJruAk4ALm/98T7gGKBa20cCZyXZou3/y76fVMvzgOXT1PlfGa7NHsCBwEkTwWkG91bVnsAHgbdX1e3Ahxjux0VVdfksjiFJWg/m+tHPqO0YPnh2AQrYfIbtDwQWDiP7AGybZGtgP4YPIKrqH5P8+8QGVfWyOa75gkmB4uKq+re2fFD7c217vTVDQPkecFtVXdfWL2d4xDEbJyX5c+Ae4LXtfPcBPj3SD08c2f5zVfUL4KaJERGG/vlkVT0C/KCNQkzlYWBiVGE5w6MxgBcCh7Tlc4C/nbxjVT2QZDHwYoYRjXOTnMAQxJ4HXNzq3RS4K8k2wNOq6vy2//8DGDmnCfsyBB2q6pYkdwATc01G+3629mVVX9yd5OvAXsB9M+z32fb3ctq9NpMkrwdeD7DzzjvPsPWMx3pM+0vSxmx9BpW/BC6pqkPbUPqlM2y/CfDbEx9qE9b2H/E2J+WBJM+cZlTl56waSdpi0ns/XcPrAO+pqg9Pqm8B8NDIqkeANT3mGXV8G3mZONa2wI+ratE024+2s7afbj+rqhqpca2uffvwvxS4NMkNwKsZPthXVtVqE4FbUHmsJl+LCSuBw9byWKPXHB593Sf6ddb9UlWnM4x+sWTJkpph85mO9Vh2l6Sx2FBfstbnz5O3A77flo+exfYXAcdOvEgy8WF9GcMETJK8FPiVWRzrPcAH2gc/SbbOql/93A4sbsuvnMWxJlwIvKaNepDkaUmeMsM+9wOz/tBuc2puS/KHrY1kmCi6JpcBhyfZtD3mOGC27TVXsaofjphqgyTPaSNjExYBdwC3Ajuk/WIpyeZJdquq+4E7kxzS1j8xyZN4dH9cDhzVttmV4bHerTPU+zXgiW1EY6K+3ds8kstZ1Rc7MIw2fbPVurDV8WTgv8zQBlPUKkkag/UZVP4GeE+Sa5ndt9Q3A0syTFS9iWEyI8C7gP2SrGQYlv/exA5TzVFpPghcAlyd5EaGD7BfjBzv5CTLGL5Bz0pVXcTwaOTKNqLwGWb+IDsT+FBmnkw76iiGx0DXM4weHDzD9ucD3wZuAj4OXDnLdiYcB7wtyQqGCb0/mWKbrRke493UtlsILK2qhxlGN97b6r2OVXNk/gh4c9v+CuDXgRXAI22C7VsZ5t9s0vrzXODoiQm702mjQocCB2b4efJKhmD6w9YXK4DrGQLN/6yqH1bVvwLnMfwC6zxWPb5bk88DhzqZVpLGKw47z29tpOPBqqokRwBHVtVM4UgjlixZUsuWreMv7pduB0unyoaS1Lck6/zoOsnyqloym23X5xwVPT4sBk7N8LDxx8BrxlyPJEm/ZFCZ59pPb2eaByNJ0lj4//qRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJEnSWquqDdKOQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKJEnqlkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUUap6U/GXcFktQ1g4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRRqnpduNuwJJ6ppBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKJEnqlkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiStB0nGXYK0UTCoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKJEnqlkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1K2xBpUkZyRZuB6Ou3mSE5N8O8k1Sa5M8tK5bmcWdRyX5EnTvHdpkluTXJ/kG0meM0dtXppkSVv+YpInz8ExN0lySpIbk9yQ5Ookz1iH4yxK8rI5qGfXdm4T1/e8JDs+1uNOauOQ9XFvSpLWzqyDSgZzGmyq6k+q6qa5PGbzl8BOwPOqak/gEGCb2e6cZLM1vV4LxwFTBpXmqKraAzgLOGkd25hWVb2sqn48B4c6HHgqsHtVPR84FFiX4y4C1iqoTHEttgD+EfhgVe3Sru9pwA7rUM+aHAIYVCRpzNYYPJIsaN/6Pw7cCDw9yQMj7x+W5My2fGb71n1Fku8mOayt3799y/9MkluSnJ0k7b3Rb/8PJHl3G2G4auIbcpJntdc3JPmr0fanqflJwOuAY6vqIYCquruqzptoZw31fyjJPwN/k2Rpkk8k+QbwiSSbJjmpjSasSPKGNZ1fkjczfLhfkuSSGa7DZcCz2/EWJ/l6kuVJLkyy00hfvTfJN5N8K8mL2/otk3wqyc1Jzge2HDm/25Ns367jzUk+kmRlkouSbNm22audz3Xt/G6cor6dgLuq6hetP++sqn9v+x/URqyuSfLpJFuPHPeKdj2/mWQ74H8Dh7e2Dk/yq0k+19q/Ksnubd/V+n5SLa8Crqyqz0+sqKpLq+rGJFsk+Vi7V65NckA73tFJTh3ply8k2X/ifph83yXZB/gD4KRW67NmuH6SpPVkNiMkuwCnVdVuVXXHDNvuBOwLvBw4cWT9CxhGFxYCzwReNMW+WwFXtRGGyxjCBsDJwMntm/ydozskuW6K4zwb+F5V3TdDrVP5DWCfqnpbe70QOLCqjgReC/ykqvYC9gJel1WPPx51flV1CvAD4ICqOmCGdl8B3JBkc+D9wGFVtRj4KPDuke02q6q9W1vvbOveBPxHVf1mW7d4mjZ2AT5QVbsxjIa8sq3/GPCGqloEPDLNvucBr2gf2n+X5AUASbYH/rz10Z7AMuBtSZ4AnAu8pV3PA4GfAn8BnFtVi6rqXOBdwLVVtTvwDuDjI22O9v2o5wHLp6nzGKDavXIkcFaGEZg1edR9V1VXABcAx7da/2XyTklen2RZkmX33HPPDE2sWRL/bIR/JM2N2TzSuKOqrprl8T7XvnXflNXnDHyzqu6EX4aLBcA/Tdr3YeALbXk58Dtt+YUMw/AA5wB/O7FD+3CdS5+uqtEP6wuq6sG2fBCwe9pIEbAdw4f/w8zu/KZydpIHgduBY4HnMHwQX9z+odsUuGtk+8+2v5e3NgD2A04BqKoVSVZM09ZtVTUR7JYDCzLMX9mmqq5s689hCJmrqao7M8yh+c/tz1eT/CHD6M1C4But3icAV7bzuKuqrm773wdM9Y/3vrTAVFVfS/JrSbZt7432/WztyxD0qKpbktwB7DrDPtPdd2tUVacDpwMsWbKk1rLOycd6LLurU4YVaW7MJqj8dNLr0X9VJ39bfWhkOdOsf2Sadn9Wq/7Fnm6b2fgOsHOSbacZVVlT/ZPPdfR1GB4nXTi6QYZHCLM5v6kcVVXLRo71ZGBlVb1wmu0n2lmX/plc45bTbTiV9hjtS8CXktzNEB4vAi6ePOqR5PlrWdtUJl+LCSuBl6zlsX7O6qOHo9d9ru47SdJ6sC6TY+9O8psZJtYeOtcFTeEqVj2mOGKmjavqP4C/B07O8AiCJDu0EQBY9/ovBN6U4fEMGX55stUM+9zPWkziBW4FdkjywtbG5kl2m2GfyxjmbZDkecDus22sTbS9P8lvtVVT9m+SPZM8tS1v0tq4g+HavCjJxPyarZLs2s5jpyR7tfXbZJgUO7k/LgeOatvsD9w7i0d25wD7JPn9kfr2a+c+erxdgZ1bLbcDizL8eunpwN4z9c0UtUqSxmBdgsoJDEPlV7D6Y4n15TiGeQ8rGOaf/GTijUw9RwWGeRP3MDyCupGh3okPwHWt/wzgJuCadswPM/O379OBL2fmybQAVNXDwGHAe5NcD1wH7DPDbh8Etk5yM8Nk1enmb0zntcBHWl9uxUj/jngK8Pl23isYRihOrap7gKOBT7brcyXw3HYehwPvb+dxMcMoxiXAwrTJtMBSYHHb90Tg1TMV2x4HvRw4NsPPk28C/pThep8GbJLkBoY5Mke3kaBvALcxXL9TgGtm0S+fAo7PMCnXybSSNCbp/fl4hl/xPFhVleQI4MiqOnjcdW0skmxdVQ+05ROAnarqLWMu63FlyZIltWzZspk3nMrS7WDpVNlQj3dJnH8kTSPJ8qpaMpttHw/P4xcDp2aYmfZj4DVjrmdj8/tJ/ozhXriDYYREkqQudB9UqupyYI9x17Gxaj8TPnfcdUiSNBX/Xz+SJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKJK0HVTXuEqSNgkFFkiR1y6AiSZK6ZVCRJEndMqhIkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd0yqEiSpG4ZVCRJUrcMKpIkqVsGFUmS1C2DiiRJ6pZBRZIkdcugIkmSumVQkSRJ3TKoSJKkbhlUJElStwwqkiSpWwYVSZLULYOKNE5LfzLuCiSpawYVSZLULYOKJEnqlkFFkiR1y6AiSZK6laoadw3S41qSe4A71nH37YF757Ccxzv7Y3X2x+rsj9U9nvvjP1XVDrPZ0KAijVGSZVW1ZNx19ML+WJ39sTr7Y3XzpT989CNJkrplUJEkSd0yqEjjdfq4C+iM/bE6+2N19sfq5kV/OEdFkiR1yxEVSZLULYOKNAZJfi/JrUm+k+SEcdczDkluT3JDkuuSLGvrfjXJxUm+3f7+lXHXub4k+WiSHyW5cWTdtOef5M/a/XJrkt8dT9XrzzT9sTTJ99s9cl2Sl428t7H3x9OTXJLkpiQrk7ylrZ9394hBRdrAkmwKfAB4KbAQODLJwvFWNTYHVNWikZ9YngB8tap2Ab7aXm+szgR+b9K6Kc+/3R9HALu1fU5r99HG5Ewe3R8A72v3yKKq+iLMm/74OfA/qmoh8NvAMe285909YlCRNry9ge9U1Xer6mHgU8DBY66pFwcDZ7Xls4BDxljLelVVlwH/Nmn1dOd/MPCpqnqoqm4DvsNwH200pumP6cyH/rirqq5py/cDNwNPYx7eIwYVacN7GvCvI6/vbOvmmwK+kmR5kte3dTtW1V1t+YfAjuMpbWymO//5fM8cm2RFezQ08ZhjXvVHkgXAC4B/Zh7eIwYVSeOyb1UtYngEdkyS/UbfrOEnifP2Z4nz/fybDwLPBBYBdwF/N95yNrwkWwP/ABxXVfeNvjdf7hGDirThfR94+sjr32jr5pWq+n77+0fA+QzD1Hcn2Qmg/f2j8VU4FtOd/7y8Z6rq7qp6pKp+AXyEVY8y5kV/JNmcIaScXVWfbavn3T1iUJE2vKuBXZI8I8kTGCbAXTDmmjaoJFsl2WZiGTgIuJGhH17dNns18H/HU+HYTHf+FwBHJHlikmcAuwDfHEN9G9TEB3JzKMM9AvOgP5IE+Hvg5qr6PyNvzbt7ZLNxFyDNN1X18yT/HbgQ2BT4aFWtHHNZG9qOwPnDv8VsBpxTVV9OcjVwXpLXMvwfqf/bGGtcr5J8Etgf2D7JncA7gROZ4vyramWS84CbGH4NckxVPTKWwteTafpj/ySLGB5v3A68AeZHfwAvAv4IuCHJdW3dO5iH94j/ZVpJktQtH/1IkqRuGVQkSVK3DCqSJKlbBhVJktQtg4okSeqWQUWSJHXLoCJJkrplUJEkSd36/6fjV4wQ3xJMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18e5b1f2ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('Current Pending Sector Count', st_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature `Current Pending Sector Count` seems to be a possible feature for predictions. Values above approx. 80 are only present for failing drives. This feature indicates the amount of \"unstable\" sectors on the hard drive.\n",
    "\n",
    "* Preferred: Low\n",
    "* Correlation to Fail by Wikipedia: Yes\n",
    "* **Correlation to Fail by given plot: Yes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18ef10044a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_feature('(Offline) Uncorrectable Sector Count', st_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This feature indicates the total count of uncorrectable errors while reading or writing. From the plot I assume a correlation between the fail of a hard drive and a value above approx. 80.\n",
    "\n",
    "* Preferred: Low\n",
    "* Correlation to Fail by Wikipedia: Yes\n",
    "* **Correlation to Fail by given plot: Yes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms and Techniques\n",
    "\n",
    "So fare we have worked with more balanced data and tried several algorithms against this datasets. But for heavy unbalanced data, I have no clue what algorithm to choose, so lets just try a lot of them and compare the results against each other based on their fbeta score! I searched all classfiers that I could found in scikit-learn and use them:\n",
    "\n",
    "\n",
    "* SVC\n",
    "* DecisionTreeClassifier\n",
    "* LinearSVC\n",
    "* SGDClassifier with max_iter=1000, tol=1e-3), # defaults in sklearn 0.21\n",
    "* NearestCentroid\n",
    "* GaussianNB,\n",
    "* AdaBoostClassifier with n_estimators=100\n",
    "* RandomForestClassifier with n_estimators=100, n_jobs=-1\n",
    "* MLPClassifier\n",
    "\n",
    "Most of this algorithms where run with their default settings. A different set of parameters could improve their results, but I wanted to have some all more or less good comparable based on their default settings.\n",
    "\n",
    "I also tried a `Keras` neuronal network, `LightGBM` and `XGBoost` to even extend the amount of algorithms that are compared. For the last 3 algorithms I used a train, test and validation set. The validation set was used by the algorithm to improve. All tests sets where run after the model was generated to calculate the fbeta score.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "Googles(Eduardo Pinheiro et al.) results are as follows (link provided above, summary see wikipedia link):\n",
    "\n",
    "* 60 days after finding uncorrectable errors (S.M.A.R.T. Value 198), the drive had 39 times higher change to fail\n",
    "First errors in S.M.A.R.T. 196 (Relocation) and 5 (offline Relocation) are strongly correlated to higher probabilities of failure\n",
    "* 56% of the drives failed without recording any count in the four string S.M.A.R.T. Warnings (Scan Errors, Relocation Count, offline Relocation and probiatinal Count)\n",
    "* 36% Failed without any S.M.A.R.T. error at all\n",
    "\n",
    "This lets me create the benchmark model as following:\n",
    "The total amount of fails is 1.84% for all drives. \n",
    "This 1.84% can now be splitted into three groups, provided using the google paper:\n",
    "\n",
    "Fails without errors: $1.84 \\cdot 0.36 = 0.6624 %%$\n",
    "\n",
    "Fails with any smart warning: $1.84 \\cdot 0.56 = 1.0304 %$\n",
    "\n",
    "Fails that could be predicted: $1.84 \\cdot (1 - 0.36 - 0.56) = 0.8 %$ \n",
    "\n",
    "The Benchmark model, using random choice, predict with a change of 0.8% that a drive will fail. Each model have different chance to fail, this means that the 0.8 % is the average probability over every model in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeAlgorithm:\n",
    "    def fit(self, X, y):\n",
    "        # Nothing to calculate\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        X - feature matrix to generate predictions for.\n",
    "        returns a list with 0 and 1 for each row in the feature matrix.\n",
    "        \"\"\"\n",
    "        y_pred = []\n",
    "        for i in range(len(X)):\n",
    "            y_pred.append(int(random.random() <= 0.08))\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology\n",
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define the base path for all zip files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'https://f001.backblazeb2.com/file/Backblaze-Hard-Drive-Data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be able to run the stuff on my own environment, I change the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(os.path.join('D:','capstone')):\n",
    "    os.chdir(os.path.join('D:','capstone'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we prepare the current directories that we will use. \n",
    "* raw - contains the raw zip files, downloaded from backblaze\n",
    "* drives - contains a csv file for each hard drive model\n",
    "* train - contains the training set for each hard drive model as csv\n",
    "* test - contains the test set for each hard drive model as csv\n",
    "* validate - contains the validation set for each hard drive model as csv\n",
    "* tmp - temp directory for splitting and normalization\n",
    "* drives_minified - removed every non relevant data and feature from a given hard drive\n",
    "* sklearn_models - containing models and results of sklearn algorithms\n",
    "* keras_models - containing models and results for keras\n",
    "* xgboost_models - containing models and results for xgboost\n",
    "* lightgbm_models - containing models and results for lightgbm\n",
    "\n",
    "When using the preprocessed data referenced in the README.md, some of this directories will already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('raw', exist_ok=True)      \n",
    "os.makedirs('drives', exist_ok=True)   \n",
    "os.makedirs('train', exist_ok=True)    \n",
    "os.makedirs('test', exist_ok=True)   \n",
    "os.makedirs('validate', exist_ok=True)\n",
    "os.makedirs('tmp', exist_ok=True)\n",
    "os.makedirs('drives_minified', exist_ok=True)\n",
    "os.makedirs('sklearn_models', exist_ok=True)\n",
    "os.makedirs('keras_models', exist_ok=True)\n",
    "os.makedirs('xgboost_models', exist_ok=True)\n",
    "os.makedirs('lightgbm_models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a list of dict's:\n",
    "* raw - the filename of the raw data\n",
    "* url - the url that the data have to be fetched from\n",
    "* zip - argument for the extraction, as the raw files for 2013, 2014 and 2015 contain a directory inside. All others are just the files directly, so we have to provide a directory to extract them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = [\n",
    "    {'raw': os.path.join('raw', 'data_Q2_2018.zip'), 'url': '{}/data_Q2_2018.zip'.format(base_path), 'zip': 'data_Q2_2018'},\n",
    "    {'raw': os.path.join('raw', 'data_Q1_2018.zip'), 'url': '{}/data_Q1_2018.zip'.format(base_path), 'zip': 'data_Q1_2018'},\n",
    "    {'raw': os.path.join('raw', 'data_Q4_2017.zip'), 'url': '{}/data_Q4_2017.zip'.format(base_path), 'zip': 'data_Q4_2017'},\n",
    "    {'raw': os.path.join('raw', 'data_Q3_2017.zip'), 'url': '{}/data_Q3_2017.zip'.format(base_path), 'zip': 'data_Q3_2017'},\n",
    "    {'raw': os.path.join('raw', 'data_Q2_2017.zip'), 'url': '{}/data_Q2_2017.zip'.format(base_path), 'zip': 'data_Q2_2017'},\n",
    "    {'raw': os.path.join('raw', 'data_Q1_2017.zip'), 'url': '{}/data_Q2_2017.zip'.format(base_path), 'zip': 'data_Q1_2017'},\n",
    "    {'raw': os.path.join('raw', 'data_Q4_2016.zip'), 'url': '{}/data_Q4_2017.zip'.format(base_path), 'zip': 'data_Q4_2016'},\n",
    "    {'raw': os.path.join('raw', 'data_Q3_2016.zip'), 'url': '{}/data_Q3_2017.zip'.format(base_path), 'zip': 'data_Q3_2016'},\n",
    "    {'raw': os.path.join('raw', 'data_Q2_2016.zip'), 'url': '{}/data_Q2_2017.zip'.format(base_path), 'zip': 'data_Q2_2016'},\n",
    "    {'raw': os.path.join('raw', 'data_Q1_2016.zip'), 'url': '{}/data_Q2_2017.zip'.format(base_path), 'zip': 'data_Q1_2016'},\n",
    "    {'raw': os.path.join('raw', 'data_2015.zip'), 'url': '{}/data_2015.zip'.format(base_path), 'zip': '2015'},\n",
    "    {'raw': os.path.join('raw', 'data_2014.zip'), 'url': '{}/data_2014.zip'.format(base_path), 'zip': '2014'},\n",
    "    {'raw': os.path.join('raw', 'data_2013.zip'), 'url': '{}/data_2013.zip'.format(base_path), 'zip': '2013'}\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 1444.43it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(records)) as pbar:\n",
    "    for record in records:\n",
    "        full_local_path = record['raw']\n",
    "        full_url = record['url']\n",
    "\n",
    "        if not os.path.exists(full_local_path):\n",
    "            print('Downloading {}'.format(full_url))\n",
    "\n",
    "            r = requests.get(full_url, stream=True)\n",
    "            with open(full_local_path, 'wb') as out_file:\n",
    "                shutil.copyfileobj(r.raw, out_file)\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:00<00:00, 13056.98it/s]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(records)) as pbar:\n",
    "    for record in records:\n",
    "        full_local_path = record['raw']\n",
    "        full_url = record['url']\n",
    "\n",
    "        if not os.path.exists(record['zip']) and not os.path.exists('data_{}'.format(record['zip'])):\n",
    "\n",
    "            print('Unpacking {}'.format(full_local_path))\n",
    "            zip_ref = zipfile.ZipFile(full_local_path, 'r')\n",
    "\n",
    "            # sometimes the zip have a directory as root, sometimes files directly\n",
    "            if not 'data' in record['zip']:\n",
    "                zip_ref.extractall()\n",
    "\n",
    "            else:\n",
    "                os.makedirs(record['zip'], exist_ok=True)\n",
    "                zip_ref.extractall(record['zip'])           \n",
    "            zip_ref.close() \n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming\n",
    "To make sure each directory extracted have a normalized name, we need to rename them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename year\n",
    "for year in [2013, 2014, 2015]:\n",
    "    if not os.path.exists('data_{}'.format(year)):\n",
    "        os.rename(str(year), 'data_{}'.format(year))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "backblaze regularly add new S.M.A.R.T. values to their generated ouput. The new values are provided somewhere in \"between\" of existing values. The header of each files is:\n",
    "* date - date the drive information was recorded\n",
    "* serial numer of the drive\n",
    "* the model\n",
    "* the capacity\n",
    "* the failure indicator\n",
    "* then for each smart value the normalized and the raw value\n",
    "\n",
    "The S.M.A.R.T. values are always in a increasing order upto 255. Adding new S.M.R.A.R.T columns, means adding them inbetween the existing ones. Reading the entire dataset is not possible, as the amount of memory required for this operation hits the 32 GB limit. Instead, the normalization have to be done directly on the files without reading everything at once.\n",
    "\n",
    "I created two methods: `extract_smart_values` and `fill_content`.\n",
    "`extract_smart_values` returns a map of an a colum to a appropriate smart value. Using this map, I can now add \"blanks\" to the dataset, to make sure that every file afterwords, have the same structure. I use `fill_content` for this approach. Both where moved to the [helper.py](helper.py) to reduce the amount of code inside this notebook a bit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by drive model\n",
    "Some S.M.A.R.T. values are manufacture specific (see wikipedia article), you can't compare them between different manufacture. Sometimes the same value can have a different meaning and finally some models inside a manufacture can be add or removed comparing to different models. This forces us to split the entire dataset by hard drive type. As we have to go over several GB of csv data, this process takes over 1h on my device... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('qualified.csv'): # this is our hint, that we don't need to rerun the preprocessing agian\n",
    "    file_handler = {}\n",
    "    total_files = glob.glob(os.path.join(\"data*\",\"*.csv\"), recursive=True)\n",
    "    with tqdm(total=len(total_files)) as pbar:\n",
    "        for file in total_files:\n",
    "            with open(file, 'rt') as f:\n",
    "                ids = []\n",
    "                for line in f:\n",
    "                    if 'date' in line:\n",
    "                        ids = Helper.extract_smart_values(line)\n",
    "                    else:\n",
    "                        parts = line.split(',')\n",
    "                        model = parts[2]\n",
    "                        content = Helper.fill_content(ids, parts)\n",
    "                        if model in file_handler:\n",
    "                            file_handler[model].write(\",\".join(content))\n",
    "                        else:\n",
    "                            file_handler[model] = open(os.path.join('tmp', '{}.csv'.format(model)), 'at')\n",
    "                            file_handler[model].write(\",\".join(content))\n",
    "\n",
    "            pbar.update(1)\n",
    "            \n",
    "    # close all file handlers\n",
    "    for handler in file_handler.values():\n",
    "        handler.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When everything went well, we can now move the files to the destination target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = glob.glob(os.path.join('tmp','*.csv'))\n",
    "with tqdm(total=len(total_files)) as pbar:\n",
    "    for file in total_files:\n",
    "        csv_name = file.split(os.sep)[-1]\n",
    "        target = os.path.join('drives', csv_name)\n",
    "        shutil.move(file, target)\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unfortunately the data is now very huge and not all models could be read into ram after the splitting and calculating the models, so I had again to now transformn the data in a minified version, removing the before created empty spaces to reduce the file size - but now the data is consistent in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate smart values that appeared the most to make sure that they are present in all rows\n",
    "# ignore lower calues\n",
    "\n",
    "def sorted_max(smarts):\n",
    "    \"\"\"\n",
    "    Input: dict with key = smart value \"column\", value = number of appearence of this smart value in the dataset.\n",
    "    Output: all keys that have a max appearence in the entire dataset for this drive. \n",
    "    This will make sure that the data is trainable by an algorithm, containing no \"nan\"s\n",
    "    \"\"\"\n",
    "    current_max = 0\n",
    "    for k, v in smarts.items():\n",
    "        # make sure to not count the \"failure\" column as smart value\n",
    "        if k > 4: \n",
    "            current_max = max(v, current_max)\n",
    "\n",
    "    # return only values with max value\n",
    "    result = []\n",
    "    for k, v in smarts.items():\n",
    "        if v == current_max and k > 4:\n",
    "            result.append(k)\n",
    "    return sorted(result)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minify(file):\n",
    "    \"\"\"\n",
    "    Minify passed file. Minify will:\n",
    "    * count the amount a smart value was present in the file\n",
    "    * this is used to get the \"important\" or \"fully filled\" features of the drives\n",
    "    * remove everything else from the file\n",
    "    \"\"\"\n",
    "    # Basic smart values that exist from the beginning of the dataset, shifed by 4, see below.\n",
    "    allowed_smart_value_index = [ 4,   5,   6,   7,   8,   9,  11,  12,  13,  14,  15,  16,  17,  19,\n",
    "       187, 188, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 227, 229, \n",
    "       244, 245, 246, 254, 255, 256, 258]\n",
    "    csv_name = file.split(os.sep)[-1]\n",
    "    tmp_path = os.path.join('tmp', csv_name)\n",
    "    target_path = os.path.join('drives_minified', csv_name)\n",
    "    if not os.path.exists(target_path):\n",
    "        \n",
    "        # Calculate all smart values that are present in the dataset for this specific drive\n",
    "        smarts = {}\n",
    "        with open(file, 'rt') as f:\n",
    "            for line in f:\n",
    "                parts = line.split(',')\n",
    "                for i, p in enumerate(parts):\n",
    "                    if len(p) > 0 and i in allowed_smart_value_index:\n",
    "                        if i in smarts:\n",
    "                            smarts[i] = smarts[i] + 1\n",
    "                        else:\n",
    "                            smarts[i] = 1\n",
    "\n",
    "        save_file_handler = open(tmp_path, 'at')\n",
    "        # write header\n",
    "        # i-4 makes sure that the parts map correctly to the smart values\n",
    "        minified_header = sorted_max(smarts)\n",
    "        header_items = ['failure'] + ['smart_{}_raw'.format(i-4) for i in minified_header]\n",
    "        save_file_handler.write(\",\".join(header_items) + '\\n')\n",
    "        with open(file, 'rt') as f:\n",
    "            for line in f:\n",
    "                minified = []\n",
    "                \n",
    "                parts = line.split(',')\n",
    "                for i, p in enumerate(parts):                  \n",
    "                    # We don't need date, serial_numer, model and capacity_bytes - so we skip index 0,1,2,3\n",
    "                    # Value 4 indicate the failure\n",
    "                    # Value [5:] Indicate the smart value from 1 ongoing\n",
    "                    if i in allowed_smart_value_index and i in minified_header and len(p)>0:\n",
    "                        minified.append(p)\n",
    "                        \n",
    "                    if i == 4:\n",
    "                        minified.append(p)\n",
    "                save_file_handler.write(\",\".join(minified) + \"\\n\")\n",
    "        save_file_handler.close()\n",
    "        shutil.move(tmp_path, target_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_files = glob.glob(os.path.join('drives','*.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now triggering the minification, took 6h on my device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 107/107 [5:56:57<00:00, 200.16s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=len(total_files)) as pbar:\n",
    "    for file in total_files:\n",
    "        minify(file)\n",
    "        pbar.update(1)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a non failure, its ok to split the data randomly. But for failed drives, I really want to make sure to have an equal distribution over the entire dataset. Before we calculate the amount of failed drives for each model, and than split them according to the failure. \n",
    "\n",
    "If we have  a failed drive, we fist move the drives to the train dataset and reduce the total amount of drives we need for it. When the train set is \"full\", we add them to the test set and finally later add them to the validation set. this makes sure that we have a equal distribution of failed drives in every set. Otherwise, their can be a chance that we have no drive inside the train, test or validation group as we are dealing with a heavely unbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drive_metrics(file):\n",
    "    \"\"\"\n",
    "    Calculate the metadata of a minified file.\n",
    "    returns a dict with\n",
    "    * \"file\" - name of the file\n",
    "    * \"failure\" - the amount of failures as int\n",
    "    * \"ok\" the amount of \"ok\" records as int\n",
    "    * \"lines\" - number of lines to check if some algorithms perform better with less or more data\n",
    "    * train - number of failures that can be used for training\n",
    "    * test - number of failures that can be used for test\n",
    "    * validate - number of failures that can be used for the validation set\n",
    "    \"\"\"\n",
    "    if len(file) == 0:\n",
    "        return {'failure':0}\n",
    "    fails = 0\n",
    "    ok = 0\n",
    "    lines = 0\n",
    "    size = os.stat(file).st_size\n",
    "    with open(file, 'rt') as f:\n",
    "        for line in f:\n",
    "            lines += 1\n",
    "            # Ignore header\n",
    "            if not 'failure' in line:\n",
    "                parts = line.split(',')\n",
    "\n",
    "                # Ignore final newline\n",
    "                if parts[0] != '\\n':\n",
    "                    failure = int(parts[0])\n",
    "                    if failure == 1:\n",
    "                        fails += 1\n",
    "                    else:\n",
    "                        ok += 1\n",
    "    if fails <= 5:\n",
    "        train =  fails - 2\n",
    "        test = 1\n",
    "        validate = 1\n",
    "    else:\n",
    "        train =  floor(fails * 0.8)\n",
    "        test = floor((fails - train) / 2)\n",
    "        validate = floor((fails - train) / 2)\n",
    "\n",
    "    return {\n",
    "        'file': file,\n",
    "        'failure':fails,\n",
    "        'ok': ok,\n",
    "        'drive_csv': file.split(os.sep)[-1],\n",
    "        'size': size,\n",
    "        'lines': lines,\n",
    "        'train': train,\n",
    "        'test': test,\n",
    "        'validate': validate,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('qualified.csv'):\n",
    "    all_drives = glob.glob(os.path.join('drives_minified', '*.csv'))\n",
    "    drive_metrics = Parallel(n_jobs=-1, backend=\"multiprocessing\", verbose=1)(delayed(drive_metrics)(file) for file in all_drives)\n",
    "    qualified_df = pd.DataFrame(drive_metrics)\n",
    "    # Only include drives that have at least 3 failure records.\n",
    "    qualified_df = qualified_df[qualified_df['failure'] >= 3]\n",
    "    # Sort by lines, as small files are less compuation heavy and the long running tasks can run over night \n",
    "    #- but debugging can be done in faster iterations\n",
    "    qualified_df.sort_values(by='lines', inplace=True)\n",
    "    qualified_df.to_csv('qualified.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As now the data is splitted, we have to make sure that the appropriate drives always include some fail drives, otherwise we can't do any training. Took after the minification only several minutes. Before the minification implemented, this took 20 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting by Train, Test and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following table now shows all our drives that we take a closer look at. As even more challenging, I also included drives that only have a small amount of fails. I assume as more failure are in general present, the more easy it is for an algorithm to generate a prediction for. We will take a look at it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Samsung SSD 850 EVO 1TB.csv</td>\n",
       "      <td>10</td>\n",
       "      <td>drives_minified\\Samsung SSD 850 EVO 1TB.csv</td>\n",
       "      <td>235</td>\n",
       "      <td>224</td>\n",
       "      <td>7791</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ST2000DL003.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\ST2000DL003.csv</td>\n",
       "      <td>1238</td>\n",
       "      <td>1229</td>\n",
       "      <td>28516</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ST2000DL001.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>drives_minified\\ST2000DL001.csv</td>\n",
       "      <td>1448</td>\n",
       "      <td>1442</td>\n",
       "      <td>36106</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ST4000DX002.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\ST4000DX002.csv</td>\n",
       "      <td>2324</td>\n",
       "      <td>2319</td>\n",
       "      <td>281200</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WDC WD1600BPVT.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD1600BPVT.csv</td>\n",
       "      <td>2495</td>\n",
       "      <td>2490</td>\n",
       "      <td>140770</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ST250LT007.csv</td>\n",
       "      <td>9</td>\n",
       "      <td>drives_minified\\ST250LT007.csv</td>\n",
       "      <td>3594</td>\n",
       "      <td>3584</td>\n",
       "      <td>516290</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WDC WD30EZRS.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\WDC WD30EZRS.csv</td>\n",
       "      <td>4425</td>\n",
       "      <td>4421</td>\n",
       "      <td>79185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ST320005XXXX.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\ST320005XXXX.csv</td>\n",
       "      <td>5033</td>\n",
       "      <td>5025</td>\n",
       "      <td>128711</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WDC WD1600AAJB.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>drives_minified\\WDC WD1600AAJB.csv</td>\n",
       "      <td>5758</td>\n",
       "      <td>5751</td>\n",
       "      <td>287064</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hitachi HDS723020BLA642.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\Hitachi HDS723020BLA642.csv</td>\n",
       "      <td>9621</td>\n",
       "      <td>9617</td>\n",
       "      <td>195003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HGST HUS726040ALE610.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\HGST HUS726040ALE610.csv</td>\n",
       "      <td>9638</td>\n",
       "      <td>9634</td>\n",
       "      <td>454674</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>WDC WD800JB.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\WDC WD800JB.csv</td>\n",
       "      <td>10384</td>\n",
       "      <td>10376</td>\n",
       "      <td>447473</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>WDC WD800AAJB.csv</td>\n",
       "      <td>11</td>\n",
       "      <td>drives_minified\\WDC WD800AAJB.csv</td>\n",
       "      <td>11019</td>\n",
       "      <td>11007</td>\n",
       "      <td>544972</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WDC WD3200BEKX.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>drives_minified\\WDC WD3200BEKX.csv</td>\n",
       "      <td>12474</td>\n",
       "      <td>12468</td>\n",
       "      <td>571737</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ST4000DM005.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>drives_minified\\ST4000DM005.csv</td>\n",
       "      <td>13291</td>\n",
       "      <td>13285</td>\n",
       "      <td>1412413</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>WDC WD800AAJS.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>drives_minified\\WDC WD800AAJS.csv</td>\n",
       "      <td>14704</td>\n",
       "      <td>14688</td>\n",
       "      <td>715873</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>WDC WD10EADX.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD10EADX.csv</td>\n",
       "      <td>15598</td>\n",
       "      <td>15593</td>\n",
       "      <td>275452</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WDC WD800BB.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\WDC WD800BB.csv</td>\n",
       "      <td>23657</td>\n",
       "      <td>23644</td>\n",
       "      <td>1057641</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ST1500DL003.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>drives_minified\\ST1500DL003.csv</td>\n",
       "      <td>30914</td>\n",
       "      <td>30823</td>\n",
       "      <td>734171</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ST9320325AS.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\ST9320325AS.csv</td>\n",
       "      <td>35270</td>\n",
       "      <td>35266</td>\n",
       "      <td>2902207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ST250LM004 HN.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\ST250LM004 HN.csv</td>\n",
       "      <td>48220</td>\n",
       "      <td>48207</td>\n",
       "      <td>3284068</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ST3160318AS.csv</td>\n",
       "      <td>16</td>\n",
       "      <td>drives_minified\\ST3160318AS.csv</td>\n",
       "      <td>48930</td>\n",
       "      <td>48913</td>\n",
       "      <td>5413401</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>WDC WD10EACS.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\WDC WD10EACS.csv</td>\n",
       "      <td>60952</td>\n",
       "      <td>60943</td>\n",
       "      <td>1108583</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ST3160316AS.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\ST3160316AS.csv</td>\n",
       "      <td>63967</td>\n",
       "      <td>63954</td>\n",
       "      <td>7067851</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>WDC WD20EFRX.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>drives_minified\\WDC WD20EFRX.csv</td>\n",
       "      <td>67423</td>\n",
       "      <td>67407</td>\n",
       "      <td>2926660</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>TOSHIBA MQ01ABF050M.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\TOSHIBA MQ01ABF050M.csv</td>\n",
       "      <td>68676</td>\n",
       "      <td>68672</td>\n",
       "      <td>3609489</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>WDC WD40EFRX.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD40EFRX.csv</td>\n",
       "      <td>71441</td>\n",
       "      <td>71436</td>\n",
       "      <td>3484180</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ST320LT007.csv</td>\n",
       "      <td>89</td>\n",
       "      <td>drives_minified\\ST320LT007.csv</td>\n",
       "      <td>72220</td>\n",
       "      <td>72130</td>\n",
       "      <td>10665867</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>TOSHIBA DT01ACA300.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\TOSHIBA DT01ACA300.csv</td>\n",
       "      <td>74178</td>\n",
       "      <td>74170</td>\n",
       "      <td>1332672</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ST9250315AS.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>drives_minified\\ST9250315AS.csv</td>\n",
       "      <td>84289</td>\n",
       "      <td>84271</td>\n",
       "      <td>6965522</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>HGST HDS5C4040ALE630.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>drives_minified\\HGST HDS5C4040ALE630.csv</td>\n",
       "      <td>87655</td>\n",
       "      <td>87648</td>\n",
       "      <td>4232731</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>ST4000DM001.csv</td>\n",
       "      <td>34</td>\n",
       "      <td>drives_minified\\ST4000DM001.csv</td>\n",
       "      <td>96120</td>\n",
       "      <td>96085</td>\n",
       "      <td>10243900</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>ST32000542AS.csv</td>\n",
       "      <td>33</td>\n",
       "      <td>drives_minified\\ST32000542AS.csv</td>\n",
       "      <td>119310</td>\n",
       "      <td>119276</td>\n",
       "      <td>3028750</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>WDC WD1600AAJS.csv</td>\n",
       "      <td>20</td>\n",
       "      <td>drives_minified\\WDC WD1600AAJS.csv</td>\n",
       "      <td>122704</td>\n",
       "      <td>122683</td>\n",
       "      <td>5648754</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>WDC WD30EZRX.csv</td>\n",
       "      <td>25</td>\n",
       "      <td>drives_minified\\WDC WD30EZRX.csv</td>\n",
       "      <td>123578</td>\n",
       "      <td>123552</td>\n",
       "      <td>2225624</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>TOSHIBA MD04ABA400V.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\TOSHIBA MD04ABA400V.csv</td>\n",
       "      <td>168000</td>\n",
       "      <td>167995</td>\n",
       "      <td>8907677</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>TOSHIBA MQ01ABF050.csv</td>\n",
       "      <td>33</td>\n",
       "      <td>drives_minified\\TOSHIBA MQ01ABF050.csv</td>\n",
       "      <td>204989</td>\n",
       "      <td>204955</td>\n",
       "      <td>10869312</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ST33000651AS.csv</td>\n",
       "      <td>31</td>\n",
       "      <td>drives_minified\\ST33000651AS.csv</td>\n",
       "      <td>222588</td>\n",
       "      <td>222556</td>\n",
       "      <td>5687995</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>HGST HUH728080ALE600.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\HGST HUH728080ALE600.csv</td>\n",
       "      <td>236609</td>\n",
       "      <td>236600</td>\n",
       "      <td>11947512</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>ST4000DX000.csv</td>\n",
       "      <td>81</td>\n",
       "      <td>drives_minified\\ST4000DX000.csv</td>\n",
       "      <td>293561</td>\n",
       "      <td>293479</td>\n",
       "      <td>7415004</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>ST31500341AS.csv</td>\n",
       "      <td>216</td>\n",
       "      <td>drives_minified\\ST31500341AS.csv</td>\n",
       "      <td>330432</td>\n",
       "      <td>330215</td>\n",
       "      <td>8653235</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ST10000NM0086.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\ST10000NM0086.csv</td>\n",
       "      <td>344216</td>\n",
       "      <td>344212</td>\n",
       "      <td>37945215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>WDC WD10EADS.csv</td>\n",
       "      <td>64</td>\n",
       "      <td>drives_minified\\WDC WD10EADS.csv</td>\n",
       "      <td>370506</td>\n",
       "      <td>370441</td>\n",
       "      <td>6740104</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>WDC WD5000LPVX.csv</td>\n",
       "      <td>45</td>\n",
       "      <td>drives_minified\\WDC WD5000LPVX.csv</td>\n",
       "      <td>400901</td>\n",
       "      <td>400855</td>\n",
       "      <td>19825397</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>WDC WD60EFRX.csv</td>\n",
       "      <td>65</td>\n",
       "      <td>drives_minified\\WDC WD60EFRX.csv</td>\n",
       "      <td>579285</td>\n",
       "      <td>579219</td>\n",
       "      <td>27132448</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>ST500LM012 HN.csv</td>\n",
       "      <td>67</td>\n",
       "      <td>drives_minified\\ST500LM012 HN.csv</td>\n",
       "      <td>775700</td>\n",
       "      <td>775632</td>\n",
       "      <td>52796831</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>WDC WD30EFRX.csv</td>\n",
       "      <td>171</td>\n",
       "      <td>drives_minified\\WDC WD30EFRX.csv</td>\n",
       "      <td>1265072</td>\n",
       "      <td>1264900</td>\n",
       "      <td>22320579</td>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Hitachi HDS723030ALA640.csv</td>\n",
       "      <td>73</td>\n",
       "      <td>drives_minified\\Hitachi HDS723030ALA640.csv</td>\n",
       "      <td>1429667</td>\n",
       "      <td>1429593</td>\n",
       "      <td>26526461</td>\n",
       "      <td>7</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>ST31500541AS.csv</td>\n",
       "      <td>397</td>\n",
       "      <td>drives_minified\\ST31500541AS.csv</td>\n",
       "      <td>1445218</td>\n",
       "      <td>1444820</td>\n",
       "      <td>37154148</td>\n",
       "      <td>40</td>\n",
       "      <td>317</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>ST3000DM001.csv</td>\n",
       "      <td>1720</td>\n",
       "      <td>drives_minified\\ST3000DM001.csv</td>\n",
       "      <td>2205149</td>\n",
       "      <td>2203428</td>\n",
       "      <td>56189869</td>\n",
       "      <td>172</td>\n",
       "      <td>1376</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>ST6000DX000.csv</td>\n",
       "      <td>65</td>\n",
       "      <td>drives_minified\\ST6000DX000.csv</td>\n",
       "      <td>2220646</td>\n",
       "      <td>2220580</td>\n",
       "      <td>275485803</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>ST12000NM0007.csv</td>\n",
       "      <td>106</td>\n",
       "      <td>drives_minified\\ST12000NM0007.csv</td>\n",
       "      <td>3422011</td>\n",
       "      <td>3421904</td>\n",
       "      <td>339870636</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Hitachi HDS5C4040ALE630.csv</td>\n",
       "      <td>88</td>\n",
       "      <td>drives_minified\\Hitachi HDS5C4040ALE630.csv</td>\n",
       "      <td>4397831</td>\n",
       "      <td>4397742</td>\n",
       "      <td>78458395</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>ST8000NM0055.csv</td>\n",
       "      <td>140</td>\n",
       "      <td>drives_minified\\ST8000NM0055.csv</td>\n",
       "      <td>5268760</td>\n",
       "      <td>5268619</td>\n",
       "      <td>569524555</td>\n",
       "      <td>14</td>\n",
       "      <td>112</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>Hitachi HDS722020ALA330.csv</td>\n",
       "      <td>235</td>\n",
       "      <td>drives_minified\\Hitachi HDS722020ALA330.csv</td>\n",
       "      <td>5306512</td>\n",
       "      <td>5306276</td>\n",
       "      <td>99028650</td>\n",
       "      <td>23</td>\n",
       "      <td>188</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>ST8000DM002.csv</td>\n",
       "      <td>187</td>\n",
       "      <td>drives_minified\\ST8000DM002.csv</td>\n",
       "      <td>6387927</td>\n",
       "      <td>6387739</td>\n",
       "      <td>698445353</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>Hitachi HDS5C3030ALA630.csv</td>\n",
       "      <td>150</td>\n",
       "      <td>drives_minified\\Hitachi HDS5C3030ALA630.csv</td>\n",
       "      <td>6641560</td>\n",
       "      <td>6641409</td>\n",
       "      <td>120329996</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>HGST HMS5C4040ALE640.csv</td>\n",
       "      <td>146</td>\n",
       "      <td>drives_minified\\HGST HMS5C4040ALE640.csv</td>\n",
       "      <td>9751915</td>\n",
       "      <td>9751768</td>\n",
       "      <td>172066594</td>\n",
       "      <td>15</td>\n",
       "      <td>116</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>HGST HMS5C4040BLE640.csv</td>\n",
       "      <td>171</td>\n",
       "      <td>drives_minified\\HGST HMS5C4040BLE640.csv</td>\n",
       "      <td>12169350</td>\n",
       "      <td>12169178</td>\n",
       "      <td>584846426</td>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>ST4000DM000.csv</td>\n",
       "      <td>3188</td>\n",
       "      <td>drives_minified\\ST4000DM000.csv</td>\n",
       "      <td>40673227</td>\n",
       "      <td>40670038</td>\n",
       "      <td>635068446</td>\n",
       "      <td>319</td>\n",
       "      <td>2550</td>\n",
       "      <td>319</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      drive_csv  failure  \\\n",
       "0   Samsung SSD 850 EVO 1TB.csv       10   \n",
       "1               ST2000DL003.csv        8   \n",
       "2               ST2000DL001.csv        5   \n",
       "3               ST4000DX002.csv        4   \n",
       "4            WDC WD1600BPVT.csv        4   \n",
       "5                ST250LT007.csv        9   \n",
       "6              WDC WD30EZRS.csv        3   \n",
       "7              ST320005XXXX.csv        7   \n",
       "8            WDC WD1600AAJB.csv        6   \n",
       "9   Hitachi HDS723020BLA642.csv        3   \n",
       "10     HGST HUS726040ALE610.csv        3   \n",
       "11              WDC WD800JB.csv        7   \n",
       "12            WDC WD800AAJB.csv       11   \n",
       "13           WDC WD3200BEKX.csv        5   \n",
       "14              ST4000DM005.csv        5   \n",
       "15            WDC WD800AAJS.csv       15   \n",
       "16             WDC WD10EADX.csv        4   \n",
       "17              WDC WD800BB.csv       12   \n",
       "18              ST1500DL003.csv       90   \n",
       "19              ST9320325AS.csv        3   \n",
       "20            ST250LM004 HN.csv       12   \n",
       "21              ST3160318AS.csv       16   \n",
       "22             WDC WD10EACS.csv        8   \n",
       "23              ST3160316AS.csv       12   \n",
       "24             WDC WD20EFRX.csv       15   \n",
       "25      TOSHIBA MQ01ABF050M.csv        3   \n",
       "26             WDC WD40EFRX.csv        4   \n",
       "27               ST320LT007.csv       89   \n",
       "28       TOSHIBA DT01ACA300.csv        7   \n",
       "29              ST9250315AS.csv       17   \n",
       "30     HGST HDS5C4040ALE630.csv        6   \n",
       "31              ST4000DM001.csv       34   \n",
       "32             ST32000542AS.csv       33   \n",
       "33           WDC WD1600AAJS.csv       20   \n",
       "34             WDC WD30EZRX.csv       25   \n",
       "35      TOSHIBA MD04ABA400V.csv        4   \n",
       "36       TOSHIBA MQ01ABF050.csv       33   \n",
       "37             ST33000651AS.csv       31   \n",
       "38     HGST HUH728080ALE600.csv        8   \n",
       "39              ST4000DX000.csv       81   \n",
       "40             ST31500341AS.csv      216   \n",
       "41            ST10000NM0086.csv        3   \n",
       "42             WDC WD10EADS.csv       64   \n",
       "43           WDC WD5000LPVX.csv       45   \n",
       "44             WDC WD60EFRX.csv       65   \n",
       "45            ST500LM012 HN.csv       67   \n",
       "46             WDC WD30EFRX.csv      171   \n",
       "47  Hitachi HDS723030ALA640.csv       73   \n",
       "48             ST31500541AS.csv      397   \n",
       "49              ST3000DM001.csv     1720   \n",
       "50              ST6000DX000.csv       65   \n",
       "51            ST12000NM0007.csv      106   \n",
       "52  Hitachi HDS5C4040ALE630.csv       88   \n",
       "53             ST8000NM0055.csv      140   \n",
       "54  Hitachi HDS722020ALA330.csv      235   \n",
       "55              ST8000DM002.csv      187   \n",
       "56  Hitachi HDS5C3030ALA630.csv      150   \n",
       "57     HGST HMS5C4040ALE640.csv      146   \n",
       "58     HGST HMS5C4040BLE640.csv      171   \n",
       "59              ST4000DM000.csv     3188   \n",
       "\n",
       "                                           file     lines        ok  \\\n",
       "0   drives_minified\\Samsung SSD 850 EVO 1TB.csv       235       224   \n",
       "1               drives_minified\\ST2000DL003.csv      1238      1229   \n",
       "2               drives_minified\\ST2000DL001.csv      1448      1442   \n",
       "3               drives_minified\\ST4000DX002.csv      2324      2319   \n",
       "4            drives_minified\\WDC WD1600BPVT.csv      2495      2490   \n",
       "5                drives_minified\\ST250LT007.csv      3594      3584   \n",
       "6              drives_minified\\WDC WD30EZRS.csv      4425      4421   \n",
       "7              drives_minified\\ST320005XXXX.csv      5033      5025   \n",
       "8            drives_minified\\WDC WD1600AAJB.csv      5758      5751   \n",
       "9   drives_minified\\Hitachi HDS723020BLA642.csv      9621      9617   \n",
       "10     drives_minified\\HGST HUS726040ALE610.csv      9638      9634   \n",
       "11              drives_minified\\WDC WD800JB.csv     10384     10376   \n",
       "12            drives_minified\\WDC WD800AAJB.csv     11019     11007   \n",
       "13           drives_minified\\WDC WD3200BEKX.csv     12474     12468   \n",
       "14              drives_minified\\ST4000DM005.csv     13291     13285   \n",
       "15            drives_minified\\WDC WD800AAJS.csv     14704     14688   \n",
       "16             drives_minified\\WDC WD10EADX.csv     15598     15593   \n",
       "17              drives_minified\\WDC WD800BB.csv     23657     23644   \n",
       "18              drives_minified\\ST1500DL003.csv     30914     30823   \n",
       "19              drives_minified\\ST9320325AS.csv     35270     35266   \n",
       "20            drives_minified\\ST250LM004 HN.csv     48220     48207   \n",
       "21              drives_minified\\ST3160318AS.csv     48930     48913   \n",
       "22             drives_minified\\WDC WD10EACS.csv     60952     60943   \n",
       "23              drives_minified\\ST3160316AS.csv     63967     63954   \n",
       "24             drives_minified\\WDC WD20EFRX.csv     67423     67407   \n",
       "25      drives_minified\\TOSHIBA MQ01ABF050M.csv     68676     68672   \n",
       "26             drives_minified\\WDC WD40EFRX.csv     71441     71436   \n",
       "27               drives_minified\\ST320LT007.csv     72220     72130   \n",
       "28       drives_minified\\TOSHIBA DT01ACA300.csv     74178     74170   \n",
       "29              drives_minified\\ST9250315AS.csv     84289     84271   \n",
       "30     drives_minified\\HGST HDS5C4040ALE630.csv     87655     87648   \n",
       "31              drives_minified\\ST4000DM001.csv     96120     96085   \n",
       "32             drives_minified\\ST32000542AS.csv    119310    119276   \n",
       "33           drives_minified\\WDC WD1600AAJS.csv    122704    122683   \n",
       "34             drives_minified\\WDC WD30EZRX.csv    123578    123552   \n",
       "35      drives_minified\\TOSHIBA MD04ABA400V.csv    168000    167995   \n",
       "36       drives_minified\\TOSHIBA MQ01ABF050.csv    204989    204955   \n",
       "37             drives_minified\\ST33000651AS.csv    222588    222556   \n",
       "38     drives_minified\\HGST HUH728080ALE600.csv    236609    236600   \n",
       "39              drives_minified\\ST4000DX000.csv    293561    293479   \n",
       "40             drives_minified\\ST31500341AS.csv    330432    330215   \n",
       "41            drives_minified\\ST10000NM0086.csv    344216    344212   \n",
       "42             drives_minified\\WDC WD10EADS.csv    370506    370441   \n",
       "43           drives_minified\\WDC WD5000LPVX.csv    400901    400855   \n",
       "44             drives_minified\\WDC WD60EFRX.csv    579285    579219   \n",
       "45            drives_minified\\ST500LM012 HN.csv    775700    775632   \n",
       "46             drives_minified\\WDC WD30EFRX.csv   1265072   1264900   \n",
       "47  drives_minified\\Hitachi HDS723030ALA640.csv   1429667   1429593   \n",
       "48             drives_minified\\ST31500541AS.csv   1445218   1444820   \n",
       "49              drives_minified\\ST3000DM001.csv   2205149   2203428   \n",
       "50              drives_minified\\ST6000DX000.csv   2220646   2220580   \n",
       "51            drives_minified\\ST12000NM0007.csv   3422011   3421904   \n",
       "52  drives_minified\\Hitachi HDS5C4040ALE630.csv   4397831   4397742   \n",
       "53             drives_minified\\ST8000NM0055.csv   5268760   5268619   \n",
       "54  drives_minified\\Hitachi HDS722020ALA330.csv   5306512   5306276   \n",
       "55              drives_minified\\ST8000DM002.csv   6387927   6387739   \n",
       "56  drives_minified\\Hitachi HDS5C3030ALA630.csv   6641560   6641409   \n",
       "57     drives_minified\\HGST HMS5C4040ALE640.csv   9751915   9751768   \n",
       "58     drives_minified\\HGST HMS5C4040BLE640.csv  12169350  12169178   \n",
       "59              drives_minified\\ST4000DM000.csv  40673227  40670038   \n",
       "\n",
       "         size  test  train  validate  \n",
       "0        7791     1      8         1  \n",
       "1       28516     1      6         1  \n",
       "2       36106     1      3         1  \n",
       "3      281200     1      2         1  \n",
       "4      140770     1      2         1  \n",
       "5      516290     1      7         1  \n",
       "6       79185     1      1         1  \n",
       "7      128711     1      5         1  \n",
       "8      287064     1      4         1  \n",
       "9      195003     1      1         1  \n",
       "10     454674     1      1         1  \n",
       "11     447473     1      5         1  \n",
       "12     544972     1      8         1  \n",
       "13     571737     1      3         1  \n",
       "14    1412413     1      3         1  \n",
       "15     715873     1     12         1  \n",
       "16     275452     1      2         1  \n",
       "17    1057641     1      9         1  \n",
       "18     734171     9     72         9  \n",
       "19    2902207     1      1         1  \n",
       "20    3284068     1      9         1  \n",
       "21    5413401     2     12         2  \n",
       "22    1108583     1      6         1  \n",
       "23    7067851     1      9         1  \n",
       "24    2926660     1     12         1  \n",
       "25    3609489     1      1         1  \n",
       "26    3484180     1      2         1  \n",
       "27   10665867     9     71         9  \n",
       "28    1332672     1      5         1  \n",
       "29    6965522     2     13         2  \n",
       "30    4232731     1      4         1  \n",
       "31   10243900     3     27         3  \n",
       "32    3028750     3     26         3  \n",
       "33    5648754     2     16         2  \n",
       "34    2225624     2     20         2  \n",
       "35    8907677     1      2         1  \n",
       "36   10869312     3     26         3  \n",
       "37    5687995     3     24         3  \n",
       "38   11947512     1      6         1  \n",
       "39    7415004     8     64         8  \n",
       "40    8653235    22    172        22  \n",
       "41   37945215     1      1         1  \n",
       "42    6740104     6     51         6  \n",
       "43   19825397     4     36         4  \n",
       "44   27132448     6     52         6  \n",
       "45   52796831     7     53         7  \n",
       "46   22320579    17    136        17  \n",
       "47   26526461     7     58         7  \n",
       "48   37154148    40    317        40  \n",
       "49   56189869   172   1376       172  \n",
       "50  275485803     6     52         6  \n",
       "51  339870636    11     84        11  \n",
       "52   78458395     9     70         9  \n",
       "53  569524555    14    112        14  \n",
       "54   99028650    23    188        23  \n",
       "55  698445353    19    149        19  \n",
       "56  120329996    15    120        15  \n",
       "57  172066594    15    116        15  \n",
       "58  584846426    17    136        17  \n",
       "59  635068446   319   2550       319  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('qualified.csv'):\n",
    "    qualified_df = pd.read_csv('qualified.csv')\n",
    "\n",
    "qualified_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `final_split` function, we will now split the extracted models into a train, test and validation set according to the calculations we made before. As the extracted fail rate can be very slow, I use the previously calculated train, test and validation counts to make sure that they are present in the appropriate set and not get \"randomly\" moved to a different set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_split(row):\n",
    "    train = row['train']\n",
    "    test = row['test']\n",
    "    validate = row['validate']\n",
    "    model = row['drive_csv']\n",
    "    file = os.path.join('drives_minified', model)\n",
    "\n",
    "    path_train_tmp = os.path.join('tmp', 'train_{}'.format(model))\n",
    "    path_test_tmp = os.path.join('tmp', 'test_{}'.format(model))    \n",
    "    path_validate_tmp = os.path.join('tmp', 'validate_{}'.format(model))      \n",
    "\n",
    "    path_train = os.path.join('train', model)\n",
    "    path_test = os.path.join('test', model)\n",
    "    path_validate = os.path.join('validate', model)\n",
    "\n",
    "\n",
    "    if not os.path.exists(path_train) and not os.path.exists(path_test) and not os.path.exists(path_validate):\n",
    "        file_handler_train =  open(path_train_tmp, 'a+')\n",
    "        file_handler_test = open(path_test_tmp, 'a+')\n",
    "        file_handler_validate = open(path_validate_tmp, 'a+')                        \n",
    "\n",
    "        with open(file, 'rt') as f:              \n",
    "            for line_index, line in enumerate(f):\n",
    "                try: \n",
    "                    # Ignore empty newline\n",
    "                    if line != '\\n':\n",
    "\n",
    "                        # Handling header, write it everywhere\n",
    "                        if line_index == 0:\n",
    "                            file_handler_train.write(line)\n",
    "                            file_handler_test.write(line)\n",
    "                            file_handler_validate.write(line)\n",
    "                        elif 'failure' in line:\n",
    "                            # ignore if double header exist as split in parallel and multiple writes with header accured\n",
    "                            continue\n",
    "                        else:\n",
    "                            parts = line.split(',')\n",
    "                            failure = int(parts[0])\n",
    "                            \n",
    "                            # Can we use random split?\n",
    "                            if failure == 0:\n",
    "                                choise = random.random()\n",
    "                                if choise >= 0.0 and choise <=0.8:\n",
    "                                    # use for training\n",
    "                                    file_handler_train.write(line)\n",
    "\n",
    "                                elif choise > 0.8 and choise <= 0.9:\n",
    "                                    # use for testing\n",
    "                                    file_handler_test.write(line)\n",
    "\n",
    "                                else:\n",
    "                                    # use for validation\n",
    "                                    file_handler_validate.write(line)\n",
    "                            # We have to do an explicit split according to the values previously calculated.\n",
    "                            else:\n",
    "                                if train > 0:\n",
    "                                    train -= 1\n",
    "                                    file_handler_train.write(line)\n",
    "                                elif test > 0:\n",
    "                                    test -= 1\n",
    "                                    file_handler_test.write(line)\n",
    "                                elif validate > 0:\n",
    "                                    validate -= 1\n",
    "                                    file_handler_validate.write(line)\n",
    "                except:\n",
    "                    raise Exception('index: {}, line: {}, file: {}'.format(line_index, line, model))\n",
    "\n",
    "\n",
    "        file_handler_train.close()\n",
    "        file_handler_test.close()\n",
    "        file_handler_validate.close()\n",
    "\n",
    "        # Make sure that the file handler is closed\n",
    "        # sleep(1000)\n",
    "        shutil.move(path_train_tmp, path_train)\n",
    "        shutil.move(path_test_tmp, path_test)\n",
    "        shutil.move(path_validate_tmp, path_validate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [07:24<00:00,  7.41s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Run this sequencially, as I got corrupted data otherwise :-|\n",
    "with tqdm(total=len(qualified_df)) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "        final_split(row)\n",
    "        pbar.update(1)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finally doing sucessfully the preprocessing, we can now start with the implementation of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Methods for more easy data preparring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dtype(model_csv):\n",
    "    \"\"\"\n",
    "    create a dict where each column is mapped to the np.float64 datatype, to make sure that the data is correcly read\n",
    "    \"\"\"\n",
    "    with open(model_csv) as f:\n",
    "        first_line = f.readline()\n",
    "        parts = first_line.split(',')\n",
    "        result = {}\n",
    "        for p in parts:\n",
    "            result[p] = np.float64\n",
    "            \n",
    "        return result       \n",
    "\n",
    "def prepare_data(X):\n",
    "    \"\"\"Cleans up the readed data in a general way. Called by prepare_* methods\"\"\"\n",
    "    X.dropna(axis='columns', inplace=True, how='all')\n",
    "    # this is needed as some values could not be extracted\n",
    "    X.dropna(axis='index', inplace=True, how='any')\n",
    "    X.rename(smart_to_name, inplace=True)\n",
    "    X.reset_index() # required?, see:https://stackoverflow.com/questions/31323499/sklearn-error-valueerror-input-contains-nan-infinity-or-a-value-too-large-for\n",
    "    y = X['failure']\n",
    "    X.drop(labels=['failure'], axis='columns', inplace=True)\n",
    "    return (X, y)\n",
    "\n",
    "def prepare_train(model_csv):\n",
    "    train_file = os.path.join('train', model_csv)\n",
    "    X_train = pd.read_csv(train_file, float_precision='high', dtype=build_dtype(train_file))\n",
    "    return prepare_data(X_train)\n",
    "\n",
    "def prepare_test(model_csv):\n",
    "    test_file = os.path.join('test', model_csv) \n",
    "    X_test = pd.read_csv(test_file, float_precision='high', dtype=build_dtype(test_file))\n",
    "    return prepare_data(X_test)\n",
    "\n",
    "def prepare_validate(model_csv):\n",
    "    validate_file = os.path.join('validate', model_csv)\n",
    "    X_validate = pd.read_csv(validate_file, float_precision='high', dtype=build_dtype(validate_file))\n",
    "    return prepare_data(X_validate)\n",
    "\n",
    "def prepare_parallel(model_csv):\n",
    "    \"\"\"Reads train, test and validation set in parallel to reduce the runtime of the training and better use IO\"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=3) as executor:\n",
    "        future_train = executor.submit(prepare_train, model_csv)\n",
    "        future_test = executor.submit(prepare_test, model_csv)\n",
    "        future_validate = executor.submit(prepare_validate, model_csv)\n",
    "        \n",
    "        # Now join everything so make sure we can process in sync\n",
    "        X_train, y_train = future_train.result()\n",
    "        X_test, y_test = future_test.result()\n",
    "        X_valid, y_valid = future_validate.result()\n",
    "        \n",
    "        assert (set(X_train.columns) == set(X_test.columns) and set(X_train.columns) == set(X_valid.columns))\n",
    "\n",
    "        return (X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "\n",
    "def run_algorithm(clf, X_train, y_train, X_test, y_test, X_valid=None, y_valid=None):\n",
    "    \"\"\"Runs a given algorithm clf on the data. Calculates the FBeta Score of the result.\"\"\"\n",
    "    if X_valid is None:\n",
    "        clf = clf.fit(X_train, y_train)\n",
    "    else:\n",
    "        clf = clf.fit(X_train, y_train, X_valid, y_valid) # use validation set for algorithm improvement if possible\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "\n",
    "    return metric(np.array(y_pred).round(), y_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_path(clf, drive):\n",
    "    \"\"\"Build the appropriate paths for model and calculated csv output according to the class that is running\"\"\"\n",
    "    clf_name = clf.__class__.__name__\n",
    "    dict_key = '{}-{}'.format(clf_name, drive)\n",
    "    \n",
    "    if 'Keras' in clf_name:\n",
    "        model_path = os.path.join('keras_models', '{}.h5'.format(dict_key))\n",
    "        result_path = os.path.join('keras_models', '{}.csv'.format(dict_key))\n",
    "        return (model_path, result_path)\n",
    "    elif 'XGBoost' in clf_name:\n",
    "        model_path = os.path.join('xgboost_models', '{}.bin'.format(dict_key))\n",
    "        result_path = os.path.join('xgboost_models', '{}.csv'.format(dict_key))\n",
    "        return (model_path, result_path)\n",
    "    elif 'LightGBM' in clf_name:\n",
    "        model_path = os.path.join('lightgbm_models', '{}.bin'.format(dict_key))\n",
    "        result_path = os.path.join('lightgbm_models', '{}.csv'.format(dict_key))\n",
    "        return (model_path, result_path)\n",
    "    else: \n",
    "        model_path = os.path.join('sklearn_models', '{}.pkl'.format(dict_key))\n",
    "        result_path =  os.path.join('sklearn_models', '{}.csv'.format(dict_key))\n",
    "        return (model_path, result_path)  \n",
    "    \n",
    "# to better use cpu power, lets to it in parallel\n",
    "# see: https://stackoverflow.com/questions/29589327/train-multiple-models-in-parallel-with-sklearn/29596675\n",
    "# and see: https://pythonhosted.org/joblib/parallel.html\n",
    "def run_parallel(clf, drive, X_train, y_train, X_test, y_test, X_valid=None, y_valid=None):\n",
    "    \"\"\"Assuming that the algorithm runs against other algorithms. Save generated model and metric data to a file.\n",
    "    Restore the data when rerun\"\"\"\n",
    "    clf_name = clf.__class__.__name__\n",
    "    dict_key = '{}-{}'.format(clf_name, drive)\n",
    "    \n",
    "    model_path, result_path = build_model_path(clf, drive)\n",
    "    # Restore data to prevent a recalculation of the same data.\n",
    "    if os.path.exists(model_path):\n",
    "        data = pd.read_csv(result_path).to_dict()\n",
    "        clf_name = data['clf_name'][0]\n",
    "        drive = data['drive'][0]\n",
    "        f_beta_score =  data['f_beta_score'][0]\n",
    "        t = data['time'][0]\n",
    "        return {'clf_name':clf_name, 'drive': drive, 'f_beta_score': f_beta_score, 'time': t}\n",
    "    else:\n",
    "        start = time()\n",
    "        f_beta_score = run_algorithm(clf, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "        finish = time() - start\n",
    "        # https://stackoverflow.com/questions/5268404/what-is-the-fastest-way-to-check-if-a-class-has-a-function-defined\n",
    "        # if present use custom save logic, otherwise use joblib\n",
    "        if hasattr(clf, \"save\"):\n",
    "            clf.save(model_path)\n",
    "        else:\n",
    "            # Dump clf for sklearn\n",
    "            joblib.dump(clf, model_path)\n",
    "        \n",
    "        # dump results\n",
    "        data = {'clf_name': clf_name, 'time': finish, 'f_beta_score': f_beta_score, 'drive': drive}\n",
    "        pd.DataFrame([data]).to_csv(result_path, index=False)\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a bunch of algorithms to get the best one, without tuning any of them. To better use the entire cpu power of the system, we run the models in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = [\n",
    "    FakeAlgorithm(),\n",
    "    DecisionTreeClassifier(),\n",
    "    LinearSVC(),\n",
    "    SGDClassifier(max_iter=1000, tol=1e-3), # defaults in sklearn 0.21\n",
    "    NearestCentroid(),\n",
    "    GaussianNB(),\n",
    "    AdaBoostClassifier(n_estimators=100),\n",
    "    RandomForestClassifier(n_estimators=100, n_jobs=-1),\n",
    "    MLPClassifier()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refinement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running all of the relevant sklearn algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "qualified_df = pd.read_csv('qualified.csv')\n",
    "\n",
    "total_items = len(qualified_df)\n",
    "clf_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ST4000DM000.csv: 100%|██████████| 60/60 [04:06<00:00, 30.31s/it]            \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=total_items) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "\n",
    "        drive_csv = row['drive_csv']\n",
    "        lines = row['lines']\n",
    "        try:\n",
    "            pbar.set_description(drive_csv)\n",
    "            \n",
    "            # I don't use prepare_parallel, as the validation set is not needed and memory is everything...\n",
    "            (X_train, y_train) =  prepare_train(drive_csv)\n",
    "            (X_test, y_test) = prepare_test(drive_csv)\n",
    "            \n",
    "            # For the biggeest drive model, not run all algorithms in parallel to prevent out of memory errors.\n",
    "            # Also only using some algorithms, as the runtime is just to huge even after runnning over night :-(\n",
    "            if lines > 20000000:\n",
    "                for clf in [FakeAlgorithm(), GaussianNB(), NearestCentroid()]:\n",
    "                    result = run_parallel(clf, drive_csv, X_train, y_train, X_test, y_test)\n",
    "                    clf_results.append(result)\n",
    "            else: \n",
    "                # Run all algorithms in parallel with shared memory to have low memory overhead - its getting warm now :-D\n",
    "                parallel_results = Parallel(n_jobs=-1, backend=\"threading\")(delayed(run_parallel)(clf, drive_csv, X_train, y_train, X_test, y_test) for clf in clfs)\n",
    "                for result in parallel_results:\n",
    "                    clf_results.append(result)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"error {} in: {}\".format(drive_csv, err))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running SVC only for all models as this seems to be the most computing intensive algorithm - running over the night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SVC for ST4000DM000.csv: 100%|██████████| 60/60 [00:02<00:00, 28.11it/s]            \n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=total_items) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "\n",
    "        drive_csv = row['drive_csv']\n",
    "        lines = row['lines']\n",
    "        try:\n",
    "            pbar.set_description('SVC for {}'.format(drive_csv))\n",
    "            # Again the calculation time is to high, skipping...\n",
    "            if lines < 50000:\n",
    "                (X_train, y_train) =  prepare_train(drive_csv)\n",
    "                (X_test, y_test) = prepare_test(drive_csv)\n",
    "                result = run_parallel(SVC(), drive_csv, X_train, y_train, X_test, y_test)\n",
    "                clf_results.append(result)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"error {} in: {}\".format(drive_csv, err))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results.csv'):\n",
    "    results_df = pd.DataFrame(list(clf_results))\n",
    "    results_df.to_csv('results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on example code by https://keras.io/getting-started/sequential-model-guide/\n",
    "class KerasAlgorithm:\n",
    "    def __init__(self, drive_csv):\n",
    "        self.model = Sequential()\n",
    "        self.batch_size = 65536\n",
    "        self.valid_model = None\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.model.to_json()\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        self.model.add(Dense(len(X_train.columns), input_dim=len(X_train.columns), activation='relu'))\n",
    "        \n",
    "        # 0.9*len, 0.8*len ... until 0.1\n",
    "        for i in range(9, 0, 1):\n",
    "            dense_size = min(2, int(len(X_train.columns) * (i/10)))\n",
    "            if dense_size == 2:\n",
    "                # Skip iteration, as network is getting to small\n",
    "                continue\n",
    "            else:\n",
    "                self.model.add(Dense(dense_size, activation='relu'))\n",
    "\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "               # used: https://machinelearningmastery.com/binary-classification-tutorial-with-the-keras-deep-learning-library/\n",
    "               # for the correct optimizer \n",
    "              optimizer='adam',\n",
    "              metrics=['binary_accuracy'])\n",
    "        self.checkpointer = ModelCheckpoint(filepath=os.path.join('keras_models', '{}.hdf5'.format(drive_csv)), \n",
    "                               verbose=0, save_best_only=True)\n",
    "        \n",
    "        # From https://www.quora.com/How-can-I-stop-training-in-Keras-if-it-does-not-improve-for-two-consecutive-epochs\n",
    "        self.early_stopping_monitor = EarlyStopping(patience=3, min_delta=0.00001)\n",
    "\n",
    "        history = self.model.fit(X_train,\n",
    "                                 y_train,\n",
    "                                 epochs=10,\n",
    "                                 batch_size=self.batch_size, \n",
    "                                 callbacks=[\n",
    "                                     self.checkpointer,\n",
    "                                     self.early_stopping_monitor],\n",
    "                                 verbose=0,\n",
    "                                 validation_data=(X_valid, y_valid))\n",
    "        if np.array(history.history['val_binary_accuracy']).sum() > 0.0:\n",
    "            self.valid_model = True\n",
    "        else:\n",
    "            self.valid_model = False\n",
    "        return self\n",
    "\n",
    "    def save(self, filename):\n",
    "            self.model.save(filename)\n",
    "\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        if not self.valid_model:\n",
    "            # using: https://stackoverflow.com/questions/5891410/numpy-array-initialization-fill-with-identical-values\n",
    "            # when the model have not enough datapoints, we can't train it and return only false for each element in the validation set\n",
    "            # this also makes sure that the fbeta score can be correctly calculated\n",
    "            # otherwise you get nan's that aren't usefull at all.\n",
    "            return np.full((1, len(X_test)), 0, dtype=int)[0]\n",
    "        else:\n",
    "            prediction = self.model.predict(X_test)\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "KerasAlgorithm for ST4000DM000.csv: 100%|██████████| 60/60 [03:41<00:00, 28.08s/it]            \n"
     ]
    }
   ],
   "source": [
    "qualified_df = pd.read_csv('qualified.csv')\n",
    "total_items = len(qualified_df)\n",
    "keras_results = []\n",
    "with tqdm(total=total_items) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "\n",
    "        drive_csv = row['drive_csv']\n",
    "        lines = row['lines']\n",
    "        try:\n",
    "            clf = KerasAlgorithm(drive_csv)\n",
    "            pbar.set_description('{} for {}'.format(clf.__class__.__name__, drive_csv))\n",
    "            (X_train, y_train, X_test, y_test, X_valid, y_valid) =  prepare_parallel(drive_csv)\n",
    "            result = run_parallel(clf, drive_csv, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "            keras_results.append(result)\n",
    "            K.clear_session()\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"error {} in: {}\".format(drive_csv, err))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results_keras.csv'):\n",
    "    results_df_keras = pd.DataFrame(keras_results)\n",
    "    results_df_keras.to_csv('results_keras.csv', index=False)\n",
    "results_df_keras = pd.read_csv('results_keras.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "class XGBoostGbtreeAlgorithm:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.num_round = 250\n",
    "        self.param = {'max_depth': 30, 'eta': 0.01, 'silent': 0, 'objective': 'binary:logistic', 'eval_metric': 'logloss'}       \n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid):\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "        evallist = [(dvalid, 'eval')]\n",
    "        self.model = xgb.train(self.param, dtrain, self.num_round, evallist, early_stopping_rounds=3, verbose_eval=0)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        dtest = xgb.DMatrix(X_test)\n",
    "        return self.model.predict(dtest, ntree_limit=self.model.best_ntree_limit).round()\n",
    "    \n",
    "    def save(self, drive_csv):\n",
    "        self.model.save_model(drive_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "XGBoostGbtreeAlgorithm for ST4000DM000.csv: 100%|██████████| 60/60 [03:46<00:00, 29.19s/it]            \n"
     ]
    }
   ],
   "source": [
    "qualified_df = pd.read_csv('qualified.csv')\n",
    "total_items = len(qualified_df)\n",
    "xgboost_results = []\n",
    "with tqdm(total=total_items) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "\n",
    "        drive_csv = row['drive_csv']\n",
    "        lines = row['lines']\n",
    "        try:\n",
    "            clf = XGBoostGbtreeAlgorithm()\n",
    "            pbar.set_description('{} for {}'.format(clf.__class__.__name__, drive_csv))\n",
    "\n",
    "            (X_train, y_train, X_test, y_test, X_valid, y_valid) =  prepare_parallel(drive_csv)\n",
    "            result = run_parallel(clf, drive_csv, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "            xgboost_results.append(result)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"error {} in: {}\".format(drive_csv, err))\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results_xgboost.csv'):\n",
    "    results_df_xgboost = pd.DataFrame(xgboost_results)\n",
    "    results_df_xgboost.to_csv('results_xgboost.csv', index=False)\n",
    "results_df_xgboost = pd.read_csv('results_xgboost.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using https://github.com/Microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\n",
    "# Use \"same\" values as xgboost for better comparing both\n",
    "class LightGBMAlgorithm:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.params = {\n",
    "            'task': 'train',\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'binary',\n",
    "            'metric': {'auc'},\n",
    "            'num_leaves': 30,\n",
    "            'learning_rate': 0.01,\n",
    "            'feature_fraction': 0.9,\n",
    "            'bagging_fraction': 0.8,\n",
    "            'bagging_freq': 5,\n",
    "            'verbose': 0\n",
    "        }\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid, y_yalid):\n",
    "        lgb_train = lgb.Dataset(X_train, y_train)\n",
    "        lgb_eval = lgb.Dataset(X_valid, y_yalid, reference=lgb_train)\n",
    "       \n",
    "        self.model = lgb.train(self.params,\n",
    "                lgb_train,\n",
    "                verbose_eval=False,\n",
    "                num_boost_round=250,\n",
    "                valid_sets=lgb_eval,\n",
    "                early_stopping_rounds=3)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # y_pred = self.model.predict(X_valid, num_iteration=self.model.best_iteration)\n",
    "        return self.model.predict(X_test, num_iteration=self.model.best_iteration).round()\n",
    "    \n",
    "    def save(self, drive_csv):\n",
    "        self.model.save_model(drive_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LightGBMAlgorithm for ST4000DM000.csv: 100%|██████████| 60/60 [03:45<00:00, 28.41s/it]            \n"
     ]
    }
   ],
   "source": [
    "qualified_df = pd.read_csv('qualified.csv')\n",
    "total_items = len(qualified_df)\n",
    "lightgbm_results = []\n",
    "with tqdm(total=total_items) as pbar:\n",
    "    for _i, row in qualified_df.iterrows():\n",
    "\n",
    "        drive_csv = row['drive_csv']\n",
    "        lines = row['lines']\n",
    "        try:\n",
    "            clf = LightGBMAlgorithm()\n",
    "            pbar.set_description('{} for {}'.format(clf.__class__.__name__, drive_csv))\n",
    "\n",
    "            (X_train, y_train, X_test, y_test, X_valid, y_valid) =  prepare_parallel(drive_csv)\n",
    "            result = run_parallel(clf, drive_csv, X_train, y_train, X_test, y_test, X_valid, y_valid)\n",
    "            lightgbm_results.append(result)\n",
    "\n",
    "        except Exception as err:\n",
    "            print(\"error {} in: {}\".format(drive_csv, err))\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('results_lightgbm.csv'):\n",
    "    results_df_lightgbm = pd.DataFrame(lightgbm_results)\n",
    "    results_df_lightgbm.to_csv('results_lightgbm.csv', index=False)\n",
    "results_df_lightgbm = pd.read_csv('results_lightgbm.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Model Evaluation and Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets take a look at the results, generated by all the run algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in glob.glob('*_models'):\n",
    "    for file in glob.glob('{}/*.csv.csv'.format(d)):\n",
    "        r = pd.read_csv(file)\n",
    "        data = r.iloc[0]\n",
    "        drive = data['drive']\n",
    "        algorithm = data['clf_name']\n",
    "        f_beta_score = data['f_beta_score']\n",
    "        runtime = data['time']\n",
    "        if not drive in all_results:\n",
    "            all_results[drive] = {}\n",
    "            all_results[drive][algorithm] = f_beta_score\n",
    "        else:\n",
    "            all_results[drive][algorithm] = f_beta_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove the nesting of the hashes, to be a valid dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = []\n",
    "for drive_csv, h in all_results.items():\n",
    "    nested_result = {'drive_csv': drive_csv}\n",
    "    for algorithm, f_beta_score in h.items():\n",
    "        nested_result[algorithm] = f_beta_score\n",
    "    final_results.append(nested_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(final_results)\n",
    "# Fill NaN with 0 for algorithms that where not run for this dataset\n",
    "results_df.fillna(0.0, inplace=True)\n",
    "qualified_df = pd.read_csv('qualified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result = results_df.merge(qualified_df, on='drive_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for _i, row in merged_result.iterrows():\n",
    "    local_winner = ''\n",
    "    local_winner_f_beta_score = 0\n",
    "    for row_name in ['AdaBoostClassifier', 'DecisionTreeClassifier', 'FakeAlgorithm','GaussianNB', 'KerasAlgorithm', 'LightGBMAlgorithm', 'LinearSVC','MLPClassifier', 'NearestCentroid', 'RandomForestClassifier','SGDClassifier', 'SVC', 'XGBoostGbtreeAlgorithm']:\n",
    "        if row[row_name] > local_winner_f_beta_score:\n",
    "            local_winner_f_beta_score = row[row_name]\n",
    "            local_winner = row_name\n",
    "    tmp.append({'drive_csv': row['drive_csv'], 'winner': local_winner, 'winner_f_beta': local_winner_f_beta_score})\n",
    "    \n",
    "merged_result = merged_result.merge(pd.DataFrame(tmp), on='drive_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To show all columns, as it is to big\n",
    "# source: https://stackoverflow.com/questions/49188960/how-do-i-show-all-of-columns-name-on-pandas-dataframe\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_result.sort_values('winner_f_beta', inplace=True)\n",
    "merged_result.to_csv('merged_result.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_range(start, end):\n",
    "    return merged_result[np.logical_and(merged_result['winner_f_beta'] > start, merged_result['winner_f_beta'] < end)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta from 0.475 to 0.482"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.475728</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.347150</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST250LT007.csv</td>\n",
       "      <td>9</td>\n",
       "      <td>drives_minified\\ST250LT007.csv</td>\n",
       "      <td>3594</td>\n",
       "      <td>3584</td>\n",
       "      <td>516290</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.475728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476431</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.397660</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD10EADX.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD10EADX.csv</td>\n",
       "      <td>15598</td>\n",
       "      <td>15593</td>\n",
       "      <td>275452</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.476431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.410908</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST9320325AS.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\ST9320325AS.csv</td>\n",
       "      <td>35270</td>\n",
       "      <td>35266</td>\n",
       "      <td>2902207</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.478370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000144</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.341463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOSHIBA MQ01ABF050M.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\TOSHIBA MQ01ABF050M.csv</td>\n",
       "      <td>68676</td>\n",
       "      <td>68672</td>\n",
       "      <td>3609489</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.478663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478756</td>\n",
       "      <td>0.144264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.406646</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HGST HUH728080ALE600.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\HGST HUH728080ALE600.csv</td>\n",
       "      <td>236609</td>\n",
       "      <td>236600</td>\n",
       "      <td>11947512</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.478756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.478870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479333</td>\n",
       "      <td>0.026703</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST33000651AS.csv</td>\n",
       "      <td>31</td>\n",
       "      <td>drives_minified\\ST33000651AS.csv</td>\n",
       "      <td>222588</td>\n",
       "      <td>222556</td>\n",
       "      <td>5687995</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.479333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.479958</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HGST HUS726040ALE610.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\HGST HUS726040ALE610.csv</td>\n",
       "      <td>9638</td>\n",
       "      <td>9634</td>\n",
       "      <td>454674</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.479958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "45                 0.0                     0.0       0.475728    0.000000   \n",
       "50                 0.0                     0.0       0.476431    0.000000   \n",
       "18                 0.0                     0.0       0.478370    0.000000   \n",
       "15                 0.0                     0.0       0.478663    0.000000   \n",
       "12                 0.0                     0.0       0.478756    0.144264   \n",
       "31                 0.0                     0.0       0.478870    0.000000   \n",
       "27                 0.0                     0.0       0.479958    0.000000   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "45        0.002639                0.0   0.000000       0.000000   \n",
       "50        0.000000                0.0   0.000000       0.000000   \n",
       "18        0.000000                0.0   0.000000       0.000000   \n",
       "15        0.000144                0.0   0.000000       0.000000   \n",
       "12        0.000000                0.0   0.000000       0.000000   \n",
       "31        0.000718                0.0   0.479333       0.026703   \n",
       "27        0.003002                0.0   0.000000       0.000000   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "45         0.347150                     0.0            0.0  0.0   \n",
       "50         0.397660                     0.0            0.0  0.0   \n",
       "18         0.410908                     0.0            0.0  0.0   \n",
       "15         0.341463                     0.0            0.0  0.0   \n",
       "12         0.406646                     0.0            0.0  0.0   \n",
       "31         0.000000                     0.0            0.0  0.0   \n",
       "27         0.372768                     0.0            0.0  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                 drive_csv  failure  \\\n",
       "45                     0.0            ST250LT007.csv        9   \n",
       "50                     0.0          WDC WD10EADX.csv        4   \n",
       "18                     0.0           ST9320325AS.csv        3   \n",
       "15                     0.0   TOSHIBA MQ01ABF050M.csv        3   \n",
       "12                     0.0  HGST HUH728080ALE600.csv        8   \n",
       "31                     0.0          ST33000651AS.csv       31   \n",
       "27                     0.0  HGST HUS726040ALE610.csv        3   \n",
       "\n",
       "                                        file   lines      ok      size  test  \\\n",
       "45            drives_minified\\ST250LT007.csv    3594    3584    516290     1   \n",
       "50          drives_minified\\WDC WD10EADX.csv   15598   15593    275452     1   \n",
       "18           drives_minified\\ST9320325AS.csv   35270   35266   2902207     1   \n",
       "15   drives_minified\\TOSHIBA MQ01ABF050M.csv   68676   68672   3609489     1   \n",
       "12  drives_minified\\HGST HUH728080ALE600.csv  236609  236600  11947512     1   \n",
       "31          drives_minified\\ST33000651AS.csv  222588  222556   5687995     3   \n",
       "27  drives_minified\\HGST HUS726040ALE610.csv    9638    9634    454674     1   \n",
       "\n",
       "    train  validate         winner  winner_f_beta  \n",
       "45      7         1  FakeAlgorithm       0.475728  \n",
       "50      2         1  FakeAlgorithm       0.476431  \n",
       "18      1         1  FakeAlgorithm       0.478370  \n",
       "15      1         1  FakeAlgorithm       0.478663  \n",
       "12      6         1  FakeAlgorithm       0.478756  \n",
       "31     24         3      LinearSVC       0.479333  \n",
       "27      1         1  FakeAlgorithm       0.479958  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0, 0.482)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided results show that for a lot of drives, the untrained `FakeAlgorithm` generates a score between 0.47 and 0.482. Only for the `ST33000651AS` the `LinearSVC` performs slightly better than the `FakeAlgorithm`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta from 0.484 to 0.497"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476102</td>\n",
       "      <td>0.484475</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.373821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD800AAJB.csv</td>\n",
       "      <td>11</td>\n",
       "      <td>drives_minified\\WDC WD800AAJB.csv</td>\n",
       "      <td>11019</td>\n",
       "      <td>11007</td>\n",
       "      <td>544972</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.484475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.486957</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.253165</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST2000DL003.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\ST2000DL003.csv</td>\n",
       "      <td>1238</td>\n",
       "      <td>1229</td>\n",
       "      <td>28516</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>FakeAlgorithm</td>\n",
       "      <td>0.486957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.480652</td>\n",
       "      <td>0.496247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.297484</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD3200BEKX.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>drives_minified\\WDC WD3200BEKX.csv</td>\n",
       "      <td>12474</td>\n",
       "      <td>12468</td>\n",
       "      <td>571737</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.496247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.476828</td>\n",
       "      <td>0.496531</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462080</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD800JB.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\WDC WD800JB.csv</td>\n",
       "      <td>10384</td>\n",
       "      <td>10376</td>\n",
       "      <td>447473</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.496531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "44                 0.0                     0.0       0.476102    0.484475   \n",
       "54                 0.0                     0.0       0.486957    0.000000   \n",
       "29                 0.0                     0.0       0.480652    0.496247   \n",
       "42                 0.0                     0.0       0.476828    0.496531   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "44        0.000885                0.0   0.000000            0.0   \n",
       "54        0.000000                0.0   0.253165            0.0   \n",
       "29        0.000000                0.0   0.000000            0.0   \n",
       "42        0.000983                0.0   0.000000            0.0   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "44         0.373821                     0.0            0.0  0.0   \n",
       "54         0.415842                     0.0            0.0  0.0   \n",
       "29         0.297484                     0.0            0.0  0.0   \n",
       "42         0.462080                     0.0            0.0  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm           drive_csv  failure  \\\n",
       "44                     0.0   WDC WD800AAJB.csv       11   \n",
       "54                     0.0     ST2000DL003.csv        8   \n",
       "29                     0.0  WDC WD3200BEKX.csv        5   \n",
       "42                     0.0     WDC WD800JB.csv        7   \n",
       "\n",
       "                                  file  lines     ok    size  test  train  \\\n",
       "44   drives_minified\\WDC WD800AAJB.csv  11019  11007  544972     1      8   \n",
       "54     drives_minified\\ST2000DL003.csv   1238   1229   28516     1      6   \n",
       "29  drives_minified\\WDC WD3200BEKX.csv  12474  12468  571737     1      3   \n",
       "42     drives_minified\\WDC WD800JB.csv  10384  10376  447473     1      5   \n",
       "\n",
       "    validate         winner  winner_f_beta  \n",
       "44         1     GaussianNB       0.484475  \n",
       "54         1  FakeAlgorithm       0.486957  \n",
       "29         1     GaussianNB       0.496247  \n",
       "42         1     GaussianNB       0.496531  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.484, 0.497)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Range, the `GaussianNB` than performs better, again with an \"outlier\" by the `FakeAlgorithm`. It is possible that for the drive `ST2000DL003` the amount of data for this drive with 1238 is just not enough to let the algorithm, learn the patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta  0.499 to 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.499027</td>\n",
       "      <td>0.499027</td>\n",
       "      <td>0.475560</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.362669</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST320005XXXX.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\ST320005XXXX.csv</td>\n",
       "      <td>5033</td>\n",
       "      <td>5025</td>\n",
       "      <td>128711</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.499139</td>\n",
       "      <td>0.499139</td>\n",
       "      <td>0.479428</td>\n",
       "      <td>0.470909</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.409137</td>\n",
       "      <td>0.499139</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD1600AAJB.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>drives_minified\\WDC WD1600AAJB.csv</td>\n",
       "      <td>5758</td>\n",
       "      <td>5751</td>\n",
       "      <td>287064</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498727</td>\n",
       "      <td>0.479683</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.249752</td>\n",
       "      <td>0.499205</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499205</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST1500DL003.csv</td>\n",
       "      <td>90</td>\n",
       "      <td>drives_minified\\ST1500DL003.csv</td>\n",
       "      <td>30914</td>\n",
       "      <td>30823</td>\n",
       "      <td>734171</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>9</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.499205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.499490</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480698</td>\n",
       "      <td>0.489340</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086512</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hitachi HDS723020BLA642.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\Hitachi HDS723020BLA642.csv</td>\n",
       "      <td>9621</td>\n",
       "      <td>9617</td>\n",
       "      <td>195003</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499613</td>\n",
       "      <td>0.476075</td>\n",
       "      <td>0.499225</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.387678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST4000DM005.csv</td>\n",
       "      <td>5</td>\n",
       "      <td>drives_minified\\ST4000DM005.csv</td>\n",
       "      <td>13291</td>\n",
       "      <td>13285</td>\n",
       "      <td>1412413</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.499654</td>\n",
       "      <td>0.499412</td>\n",
       "      <td>0.478364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227266</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.349604</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST320LT007.csv</td>\n",
       "      <td>89</td>\n",
       "      <td>drives_minified\\ST320LT007.csv</td>\n",
       "      <td>72220</td>\n",
       "      <td>72130</td>\n",
       "      <td>10665867</td>\n",
       "      <td>9</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499680</td>\n",
       "      <td>0.480479</td>\n",
       "      <td>0.496344</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.423843</td>\n",
       "      <td>0.499786</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD800BB.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\WDC WD800BB.csv</td>\n",
       "      <td>23657</td>\n",
       "      <td>23644</td>\n",
       "      <td>1057641</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.499786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499845</td>\n",
       "      <td>0.480548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.495749</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.296624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST4000DM001.csv</td>\n",
       "      <td>34</td>\n",
       "      <td>drives_minified\\ST4000DM001.csv</td>\n",
       "      <td>96120</td>\n",
       "      <td>96085</td>\n",
       "      <td>10243900</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499849</td>\n",
       "      <td>0.479243</td>\n",
       "      <td>0.499798</td>\n",
       "      <td>0.497923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.340341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST3160318AS.csv</td>\n",
       "      <td>16</td>\n",
       "      <td>drives_minified\\ST3160318AS.csv</td>\n",
       "      <td>48930</td>\n",
       "      <td>48913</td>\n",
       "      <td>5413401</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479456</td>\n",
       "      <td>0.499877</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386811</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD10EACS.csv</td>\n",
       "      <td>8</td>\n",
       "      <td>drives_minified\\WDC WD10EACS.csv</td>\n",
       "      <td>60952</td>\n",
       "      <td>60943</td>\n",
       "      <td>1108583</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.499877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499889</td>\n",
       "      <td>0.479679</td>\n",
       "      <td>0.488842</td>\n",
       "      <td>0.499182</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499480</td>\n",
       "      <td>0.398804</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD20EFRX.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>drives_minified\\WDC WD20EFRX.csv</td>\n",
       "      <td>67423</td>\n",
       "      <td>67407</td>\n",
       "      <td>2926660</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499895</td>\n",
       "      <td>0.478194</td>\n",
       "      <td>0.495828</td>\n",
       "      <td>0.496095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.271297</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST250LM004 HN.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\ST250LM004 HN.csv</td>\n",
       "      <td>48220</td>\n",
       "      <td>48207</td>\n",
       "      <td>3284068</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499911</td>\n",
       "      <td>0.480173</td>\n",
       "      <td>0.246326</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.487006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.323150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST9250315AS.csv</td>\n",
       "      <td>17</td>\n",
       "      <td>drives_minified\\ST9250315AS.csv</td>\n",
       "      <td>84289</td>\n",
       "      <td>84271</td>\n",
       "      <td>6965522</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499916</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499895</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST32000542AS.csv</td>\n",
       "      <td>33</td>\n",
       "      <td>drives_minified\\ST32000542AS.csv</td>\n",
       "      <td>119310</td>\n",
       "      <td>119276</td>\n",
       "      <td>3028750</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499922</td>\n",
       "      <td>0.482247</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.066065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.334651</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST3160316AS.csv</td>\n",
       "      <td>12</td>\n",
       "      <td>drives_minified\\ST3160316AS.csv</td>\n",
       "      <td>63967</td>\n",
       "      <td>63954</td>\n",
       "      <td>7067851</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.499922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.499933</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479936</td>\n",
       "      <td>0.499330</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499866</td>\n",
       "      <td>0.499465</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.498457</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOSHIBA DT01ACA300.csv</td>\n",
       "      <td>7</td>\n",
       "      <td>drives_minified\\TOSHIBA DT01ACA300.csv</td>\n",
       "      <td>74178</td>\n",
       "      <td>74170</td>\n",
       "      <td>1332672</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.478980</td>\n",
       "      <td>0.482234</td>\n",
       "      <td>0.050693</td>\n",
       "      <td>0.499886</td>\n",
       "      <td>0.499430</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.014364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HGST HDS5C4040ALE630.csv</td>\n",
       "      <td>6</td>\n",
       "      <td>drives_minified\\HGST HDS5C4040ALE630.csv</td>\n",
       "      <td>87655</td>\n",
       "      <td>87648</td>\n",
       "      <td>4232731</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>SGDClassifier</td>\n",
       "      <td>0.499943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499938</td>\n",
       "      <td>0.478961</td>\n",
       "      <td>0.499371</td>\n",
       "      <td>0.498395</td>\n",
       "      <td>0.499913</td>\n",
       "      <td>0.499969</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.227440</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD5000LPVX.csv</td>\n",
       "      <td>45</td>\n",
       "      <td>drives_minified\\WDC WD5000LPVX.csv</td>\n",
       "      <td>400901</td>\n",
       "      <td>400855</td>\n",
       "      <td>19825397</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.499969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.499986</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.424392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ST10000NM0086.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\ST10000NM0086.csv</td>\n",
       "      <td>344216</td>\n",
       "      <td>344212</td>\n",
       "      <td>37945215</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.499986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "57            0.499027                0.499027       0.475560    0.000000   \n",
       "43            0.499139                0.499139       0.479428    0.470909   \n",
       "52            0.000000                0.498727       0.479683    0.000000   \n",
       "40            0.499490                0.000000       0.480698    0.489340   \n",
       "34            0.000000                0.499613       0.476075    0.499225   \n",
       "24            0.499654                0.499412       0.478364    0.000000   \n",
       "46            0.000000                0.499680       0.480479    0.496344   \n",
       "14            0.000000                0.499845       0.480548    0.000000   \n",
       "28            0.000000                0.499849       0.479243    0.499798   \n",
       "49            0.000000                0.000000       0.479456    0.499877   \n",
       "39            0.000000                0.499889       0.479679    0.488842   \n",
       "25            0.000000                0.499895       0.478194    0.495828   \n",
       "16            0.000000                0.499911       0.480173    0.246326   \n",
       "53            0.000000                0.499916       0.479727    0.000000   \n",
       "30            0.000000                0.499922       0.482247    0.000000   \n",
       "38            0.499933                0.000000       0.479936    0.499330   \n",
       "21            0.499915                0.499915       0.478980    0.482234   \n",
       "6             0.000000                0.499938       0.478961    0.499371   \n",
       "9             0.499986                0.000000       0.478558    0.000000   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "57        0.000000           0.000000   0.000000       0.000000   \n",
       "43        0.000000           0.000000   0.000000       0.000000   \n",
       "52        0.000000           0.000000   0.000000       0.000000   \n",
       "40        0.000000           0.000000   0.000000       0.000000   \n",
       "34        0.000773           0.000000   0.000000       0.000000   \n",
       "24        0.000000           0.000000   0.227266       0.000000   \n",
       "46        0.000000           0.000000   0.000000       0.000000   \n",
       "14        0.000000           0.000000   0.495749       0.000000   \n",
       "28        0.497923           0.000000   0.000000       0.000000   \n",
       "49        0.000164           0.000000   0.000000       0.000000   \n",
       "39        0.499182           0.000000   0.000000       0.499480   \n",
       "25        0.496095           0.000000   0.000000       0.000000   \n",
       "16        0.000000           0.000000   0.487006       0.000000   \n",
       "53        0.499895           0.000000   0.000000       0.000000   \n",
       "30        0.000469           0.000000   0.066065       0.000000   \n",
       "38        0.000000           0.499866   0.499465       0.000000   \n",
       "21        0.050693           0.499886   0.499430       0.499915   \n",
       "6         0.498395           0.499913   0.499969       0.000000   \n",
       "9         0.000029           0.000000   0.000000       0.000000   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier       SVC  \\\n",
       "57         0.362669                0.000000       0.000000  0.000000   \n",
       "43         0.409137                0.499139       0.000000  0.000000   \n",
       "52         0.249752                0.499205       0.000000  0.499205   \n",
       "40         0.086512                0.000000       0.000000  0.000000   \n",
       "34         0.387678                0.000000       0.000000  0.000000   \n",
       "24         0.349604                0.000000       0.000000  0.000000   \n",
       "46         0.423843                0.499786       0.000000  0.000000   \n",
       "14         0.296624                0.000000       0.000000  0.000000   \n",
       "28         0.340341                0.000000       0.000000  0.000000   \n",
       "49         0.386811                0.000000       0.000000  0.000000   \n",
       "39         0.398804                0.000000       0.000000  0.000000   \n",
       "25         0.271297                0.000000       0.000000  0.000000   \n",
       "16         0.323150                0.000000       0.000000  0.000000   \n",
       "53         0.334286                0.000000       0.000000  0.000000   \n",
       "30         0.334651                0.000000       0.000000  0.000000   \n",
       "38         0.498457                0.000000       0.000000  0.000000   \n",
       "21         0.014364                0.000000       0.499943  0.000000   \n",
       "6          0.227440                0.000000       0.000000  0.000000   \n",
       "9          0.424392                0.000000       0.000000  0.000000   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                    drive_csv  failure  \\\n",
       "57                     0.0             ST320005XXXX.csv        7   \n",
       "43                     0.0           WDC WD1600AAJB.csv        6   \n",
       "52                     0.0              ST1500DL003.csv       90   \n",
       "40                     0.0  Hitachi HDS723020BLA642.csv        3   \n",
       "34                     0.0              ST4000DM005.csv        5   \n",
       "24                     0.0               ST320LT007.csv       89   \n",
       "46                     0.0              WDC WD800BB.csv       12   \n",
       "14                     0.0              ST4000DM001.csv       34   \n",
       "28                     0.0              ST3160318AS.csv       16   \n",
       "49                     0.0             WDC WD10EACS.csv        8   \n",
       "39                     0.0             WDC WD20EFRX.csv       15   \n",
       "25                     0.0            ST250LM004 HN.csv       12   \n",
       "16                     0.0              ST9250315AS.csv       17   \n",
       "53                     0.0             ST32000542AS.csv       33   \n",
       "30                     0.0              ST3160316AS.csv       12   \n",
       "38                     0.0       TOSHIBA DT01ACA300.csv        7   \n",
       "21                     0.0     HGST HDS5C4040ALE630.csv        6   \n",
       "6                      0.0           WDC WD5000LPVX.csv       45   \n",
       "9                      0.0            ST10000NM0086.csv        3   \n",
       "\n",
       "                                           file   lines      ok      size  \\\n",
       "57             drives_minified\\ST320005XXXX.csv    5033    5025    128711   \n",
       "43           drives_minified\\WDC WD1600AAJB.csv    5758    5751    287064   \n",
       "52              drives_minified\\ST1500DL003.csv   30914   30823    734171   \n",
       "40  drives_minified\\Hitachi HDS723020BLA642.csv    9621    9617    195003   \n",
       "34              drives_minified\\ST4000DM005.csv   13291   13285   1412413   \n",
       "24               drives_minified\\ST320LT007.csv   72220   72130  10665867   \n",
       "46              drives_minified\\WDC WD800BB.csv   23657   23644   1057641   \n",
       "14              drives_minified\\ST4000DM001.csv   96120   96085  10243900   \n",
       "28              drives_minified\\ST3160318AS.csv   48930   48913   5413401   \n",
       "49             drives_minified\\WDC WD10EACS.csv   60952   60943   1108583   \n",
       "39             drives_minified\\WDC WD20EFRX.csv   67423   67407   2926660   \n",
       "25            drives_minified\\ST250LM004 HN.csv   48220   48207   3284068   \n",
       "16              drives_minified\\ST9250315AS.csv   84289   84271   6965522   \n",
       "53             drives_minified\\ST32000542AS.csv  119310  119276   3028750   \n",
       "30              drives_minified\\ST3160316AS.csv   63967   63954   7067851   \n",
       "38       drives_minified\\TOSHIBA DT01ACA300.csv   74178   74170   1332672   \n",
       "21     drives_minified\\HGST HDS5C4040ALE630.csv   87655   87648   4232731   \n",
       "6            drives_minified\\WDC WD5000LPVX.csv  400901  400855  19825397   \n",
       "9             drives_minified\\ST10000NM0086.csv  344216  344212  37945215   \n",
       "\n",
       "    test  train  validate                  winner  winner_f_beta  \n",
       "57     1      5         1      AdaBoostClassifier       0.499027  \n",
       "43     1      4         1      AdaBoostClassifier       0.499139  \n",
       "52     9     72         9  RandomForestClassifier       0.499205  \n",
       "40     1      1         1      AdaBoostClassifier       0.499490  \n",
       "34     1      3         1  DecisionTreeClassifier       0.499613  \n",
       "24     9     71         9      AdaBoostClassifier       0.499654  \n",
       "46     1      9         1  RandomForestClassifier       0.499786  \n",
       "14     3     27         3  DecisionTreeClassifier       0.499845  \n",
       "28     2     12         2  DecisionTreeClassifier       0.499849  \n",
       "49     1      6         1              GaussianNB       0.499877  \n",
       "39     1     12         1  DecisionTreeClassifier       0.499889  \n",
       "25     1      9         1  DecisionTreeClassifier       0.499895  \n",
       "16     2     13         2  DecisionTreeClassifier       0.499911  \n",
       "53     3     26         3  DecisionTreeClassifier       0.499916  \n",
       "30     1      9         1  DecisionTreeClassifier       0.499922  \n",
       "38     1      5         1      AdaBoostClassifier       0.499933  \n",
       "21     1      4         1           SGDClassifier       0.499943  \n",
       "6      4     36         4               LinearSVC       0.499969  \n",
       "9      1      1         1      AdaBoostClassifier       0.499986  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.499, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of different algorithms win with this fbeta score.\n",
    "\n",
    "|Algorithm|Wins|\n",
    "|-----|---|\n",
    "|AdaBoostClassifier|6|\n",
    "|RandomForestClassifier|2|\n",
    "|DecisionTreeClassifier|7|\n",
    "|GaussianNB|1|\n",
    "|SGDClassifier|1|\n",
    "|LinearSVC|1|\n",
    "\n",
    "This Range is the area of the somehow \"Tree\" based approaches, again with some outliers by `GaussianNB`, `SGDClassifier` and `LinearSVC`. All this drives outperform the random guess, they could detect some patterns inside the data, even with a low fbeta score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fbeta from 0.5 to 0.52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499905</td>\n",
       "      <td>0.478769</td>\n",
       "      <td>0.502979</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.499952</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502693</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD10EADS.csv</td>\n",
       "      <td>64</td>\n",
       "      <td>drives_minified\\WDC WD10EADS.csv</td>\n",
       "      <td>370506</td>\n",
       "      <td>370441</td>\n",
       "      <td>6740104</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>6</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.502979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.479330</td>\n",
       "      <td>0.502965</td>\n",
       "      <td>0.499987</td>\n",
       "      <td>0.499977</td>\n",
       "      <td>0.50315</td>\n",
       "      <td>0.492148</td>\n",
       "      <td>0.503572</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Hitachi HDS722020ALA330.csv</td>\n",
       "      <td>235</td>\n",
       "      <td>drives_minified\\Hitachi HDS722020ALA330.csv</td>\n",
       "      <td>5306512</td>\n",
       "      <td>5306276</td>\n",
       "      <td>99028650</td>\n",
       "      <td>23</td>\n",
       "      <td>188</td>\n",
       "      <td>23</td>\n",
       "      <td>NearestCentroid</td>\n",
       "      <td>0.503572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499914</td>\n",
       "      <td>0.478585</td>\n",
       "      <td>0.509025</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.435136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOSHIBA MQ01ABF050.csv</td>\n",
       "      <td>33</td>\n",
       "      <td>drives_minified\\TOSHIBA MQ01ABF050.csv</td>\n",
       "      <td>204989</td>\n",
       "      <td>204955</td>\n",
       "      <td>10869312</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.509025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.479727</td>\n",
       "      <td>0.518725</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD40EFRX.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD40EFRX.csv</td>\n",
       "      <td>71441</td>\n",
       "      <td>71436</td>\n",
       "      <td>3484180</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.518725</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "48                 0.0                0.499905       0.478769    0.502979   \n",
       "33                 0.0                0.499983       0.479330    0.502965   \n",
       "7                  0.0                0.499914       0.478585    0.509025   \n",
       "23                 0.0                0.000000       0.479727    0.518725   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "48        0.000163           0.499952    0.00000       0.000000   \n",
       "33        0.499987           0.499977    0.50315       0.492148   \n",
       "7         0.000147           0.000000    0.00000       0.000000   \n",
       "23        0.000143           0.000000    0.00000       0.000000   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "48         0.502693                     0.0            0.0  0.0   \n",
       "33         0.503572                     0.0            0.0  0.0   \n",
       "7          0.435136                     0.0            0.0  0.0   \n",
       "23         0.375913                     0.0            0.0  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                    drive_csv  failure  \\\n",
       "48                     0.0             WDC WD10EADS.csv       64   \n",
       "33                     0.0  Hitachi HDS722020ALA330.csv      235   \n",
       "7                      0.0       TOSHIBA MQ01ABF050.csv       33   \n",
       "23                     0.0             WDC WD40EFRX.csv        4   \n",
       "\n",
       "                                           file    lines       ok      size  \\\n",
       "48             drives_minified\\WDC WD10EADS.csv   370506   370441   6740104   \n",
       "33  drives_minified\\Hitachi HDS722020ALA330.csv  5306512  5306276  99028650   \n",
       "7        drives_minified\\TOSHIBA MQ01ABF050.csv   204989   204955  10869312   \n",
       "23             drives_minified\\WDC WD40EFRX.csv    71441    71436   3484180   \n",
       "\n",
       "    test  train  validate           winner  winner_f_beta  \n",
       "48     6     51         6       GaussianNB       0.502979  \n",
       "33    23    188        23  NearestCentroid       0.503572  \n",
       "7      3     26         3       GaussianNB       0.509025  \n",
       "23     1      2         1       GaussianNB       0.518725  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.5,0.52)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next range is leading by `GaussianNB` again. Only for the `Hitachi HDS722020ALA330` the `NearestCentroid` algorithm wins. `GaussianNB` have a Delta of only 0.000607 - so it was really close compared to the  `NearestCentroid` algorithm. A rerun of this dataset could already generate a different score and a different winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta from 0.52 to 0.55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.499801</td>\n",
       "      <td>0.499574</td>\n",
       "      <td>0.480095</td>\n",
       "      <td>0.516997</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526401</td>\n",
       "      <td>0.515153</td>\n",
       "      <td>0.333388</td>\n",
       "      <td>0.499804</td>\n",
       "      <td>0.499766</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>ST3000DM001.csv</td>\n",
       "      <td>1720</td>\n",
       "      <td>drives_minified\\ST3000DM001.csv</td>\n",
       "      <td>2205149</td>\n",
       "      <td>2203428</td>\n",
       "      <td>56189869</td>\n",
       "      <td>172</td>\n",
       "      <td>1376</td>\n",
       "      <td>172</td>\n",
       "      <td>LinearSVC</td>\n",
       "      <td>0.526401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.519144</td>\n",
       "      <td>0.528457</td>\n",
       "      <td>0.479389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.499915</td>\n",
       "      <td>0.499912</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336368</td>\n",
       "      <td>0.499924</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499924</td>\n",
       "      <td>ST31500541AS.csv</td>\n",
       "      <td>397</td>\n",
       "      <td>drives_minified\\ST31500541AS.csv</td>\n",
       "      <td>1445218</td>\n",
       "      <td>1444820</td>\n",
       "      <td>37154148</td>\n",
       "      <td>40</td>\n",
       "      <td>317</td>\n",
       "      <td>40</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.528457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499939</td>\n",
       "      <td>0.478691</td>\n",
       "      <td>0.497923</td>\n",
       "      <td>0.539065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.351871</td>\n",
       "      <td>0.499939</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>WDC WD1600AAJS.csv</td>\n",
       "      <td>20</td>\n",
       "      <td>drives_minified\\WDC WD1600AAJS.csv</td>\n",
       "      <td>122704</td>\n",
       "      <td>122683</td>\n",
       "      <td>5648754</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>KerasAlgorithm</td>\n",
       "      <td>0.539065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.538279</td>\n",
       "      <td>0.499688</td>\n",
       "      <td>0.478982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.339870</td>\n",
       "      <td>0.499825</td>\n",
       "      <td>0.499810</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.543318</td>\n",
       "      <td>ST31500341AS.csv</td>\n",
       "      <td>216</td>\n",
       "      <td>drives_minified\\ST31500341AS.csv</td>\n",
       "      <td>330432</td>\n",
       "      <td>330215</td>\n",
       "      <td>8653235</td>\n",
       "      <td>22</td>\n",
       "      <td>172</td>\n",
       "      <td>22</td>\n",
       "      <td>XGBoostGbtreeAlgorithm</td>\n",
       "      <td>0.543318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.547614</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.479077</td>\n",
       "      <td>0.504739</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.523245</td>\n",
       "      <td>0.505784</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.506405</td>\n",
       "      <td>0.499995</td>\n",
       "      <td>0.499993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>HGST HMS5C4040ALE640.csv</td>\n",
       "      <td>146</td>\n",
       "      <td>drives_minified\\HGST HMS5C4040ALE640.csv</td>\n",
       "      <td>9751915</td>\n",
       "      <td>9751768</td>\n",
       "      <td>172066594</td>\n",
       "      <td>15</td>\n",
       "      <td>116</td>\n",
       "      <td>15</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.547614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "51            0.499801                0.499574       0.480095    0.516997   \n",
       "19            0.519144                0.528457       0.479389    0.000000   \n",
       "20            0.000000                0.499939       0.478691    0.497923   \n",
       "47            0.538279                0.499688       0.478982    0.000000   \n",
       "0             0.547614                0.499994       0.479077    0.504739   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "51        0.000780           0.000000   0.526401       0.515153   \n",
       "19        0.000768           0.499915   0.499912       0.000000   \n",
       "20        0.539065           0.000000   0.000000       0.000000   \n",
       "47        0.000000           0.000000   0.000000       0.000000   \n",
       "0         0.000000           0.523245   0.505784       0.499996   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "51         0.333388                0.499804       0.499766  0.0   \n",
       "19         0.336368                0.499924       0.000000  0.0   \n",
       "20         0.351871                0.499939       0.000000  0.0   \n",
       "47         0.339870                0.499825       0.499810  0.0   \n",
       "0          0.506405                0.499995       0.499993  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                 drive_csv  failure  \\\n",
       "51                0.000000           ST3000DM001.csv     1720   \n",
       "19                0.499924          ST31500541AS.csv      397   \n",
       "20                0.000000        WDC WD1600AAJS.csv       20   \n",
       "47                0.543318          ST31500341AS.csv      216   \n",
       "0                 0.000000  HGST HMS5C4040ALE640.csv      146   \n",
       "\n",
       "                                        file    lines       ok       size  \\\n",
       "51           drives_minified\\ST3000DM001.csv  2205149  2203428   56189869   \n",
       "19          drives_minified\\ST31500541AS.csv  1445218  1444820   37154148   \n",
       "20        drives_minified\\WDC WD1600AAJS.csv   122704   122683    5648754   \n",
       "47          drives_minified\\ST31500341AS.csv   330432   330215    8653235   \n",
       "0   drives_minified\\HGST HMS5C4040ALE640.csv  9751915  9751768  172066594   \n",
       "\n",
       "    test  train  validate                  winner  winner_f_beta  \n",
       "51   172   1376       172               LinearSVC       0.526401  \n",
       "19    40    317        40  DecisionTreeClassifier       0.528457  \n",
       "20     2     16         2          KerasAlgorithm       0.539065  \n",
       "47    22    172        22  XGBoostGbtreeAlgorithm       0.543318  \n",
       "0     15    116        15      AdaBoostClassifier       0.547614  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.52, 0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Range is totally mixed. Each winner is only one time present. We also have `Keras` and `XGBoost` first time as winners for a drive. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta from 0.55 to 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.502903</td>\n",
       "      <td>0.532582</td>\n",
       "      <td>0.479273</td>\n",
       "      <td>0.522308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.550953</td>\n",
       "      <td>0.528401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.369063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.520629</td>\n",
       "      <td>ST4000DM000.csv</td>\n",
       "      <td>3188</td>\n",
       "      <td>drives_minified\\ST4000DM000.csv</td>\n",
       "      <td>40673227</td>\n",
       "      <td>40670038</td>\n",
       "      <td>635068446</td>\n",
       "      <td>319</td>\n",
       "      <td>2550</td>\n",
       "      <td>319</td>\n",
       "      <td>LightGBMAlgorithm</td>\n",
       "      <td>0.550953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499953</td>\n",
       "      <td>0.479579</td>\n",
       "      <td>0.536435</td>\n",
       "      <td>0.551913</td>\n",
       "      <td>0.499962</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.350368</td>\n",
       "      <td>0.499959</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>WDC WD30EFRX.csv</td>\n",
       "      <td>171</td>\n",
       "      <td>drives_minified\\WDC WD30EFRX.csv</td>\n",
       "      <td>1265072</td>\n",
       "      <td>1264900</td>\n",
       "      <td>22320579</td>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>KerasAlgorithm</td>\n",
       "      <td>0.551913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.566657</td>\n",
       "      <td>0.479213</td>\n",
       "      <td>0.497455</td>\n",
       "      <td>0.085084</td>\n",
       "      <td>0.535704</td>\n",
       "      <td>0.495267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.477563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.547612</td>\n",
       "      <td>ST8000DM002.csv</td>\n",
       "      <td>187</td>\n",
       "      <td>drives_minified\\ST8000DM002.csv</td>\n",
       "      <td>6387927</td>\n",
       "      <td>6387739</td>\n",
       "      <td>698445353</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>19</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.566657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.549991</td>\n",
       "      <td>0.538450</td>\n",
       "      <td>0.479053</td>\n",
       "      <td>0.499024</td>\n",
       "      <td>0.490756</td>\n",
       "      <td>0.511323</td>\n",
       "      <td>0.499951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.498328</td>\n",
       "      <td>0.566660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.566660</td>\n",
       "      <td>ST8000NM0055.csv</td>\n",
       "      <td>140</td>\n",
       "      <td>drives_minified\\ST8000NM0055.csv</td>\n",
       "      <td>5268760</td>\n",
       "      <td>5268619</td>\n",
       "      <td>569524555</td>\n",
       "      <td>14</td>\n",
       "      <td>112</td>\n",
       "      <td>14</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.566660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.583326</td>\n",
       "      <td>0.499988</td>\n",
       "      <td>0.479191</td>\n",
       "      <td>0.499895</td>\n",
       "      <td>0.498826</td>\n",
       "      <td>0.499974</td>\n",
       "      <td>0.499806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.341959</td>\n",
       "      <td>0.499991</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499991</td>\n",
       "      <td>ST12000NM0007.csv</td>\n",
       "      <td>106</td>\n",
       "      <td>drives_minified\\ST12000NM0007.csv</td>\n",
       "      <td>3422011</td>\n",
       "      <td>3421904</td>\n",
       "      <td>339870636</td>\n",
       "      <td>11</td>\n",
       "      <td>84</td>\n",
       "      <td>11</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.583326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.583329</td>\n",
       "      <td>0.532252</td>\n",
       "      <td>0.479184</td>\n",
       "      <td>0.502323</td>\n",
       "      <td>0.501172</td>\n",
       "      <td>0.518486</td>\n",
       "      <td>0.499993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.503717</td>\n",
       "      <td>0.499996</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.555552</td>\n",
       "      <td>HGST HMS5C4040BLE640.csv</td>\n",
       "      <td>171</td>\n",
       "      <td>drives_minified\\HGST HMS5C4040BLE640.csv</td>\n",
       "      <td>12169350</td>\n",
       "      <td>12169178</td>\n",
       "      <td>584846426</td>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>17</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.583329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.499924</td>\n",
       "      <td>0.547458</td>\n",
       "      <td>0.479706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.337198</td>\n",
       "      <td>0.599932</td>\n",
       "      <td>0.499881</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.599932</td>\n",
       "      <td>ST4000DX000.csv</td>\n",
       "      <td>81</td>\n",
       "      <td>drives_minified\\ST4000DX000.csv</td>\n",
       "      <td>293561</td>\n",
       "      <td>293479</td>\n",
       "      <td>7415004</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>RandomForestClassifier</td>\n",
       "      <td>0.599932</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "59            0.502903                0.532582       0.479273    0.522308   \n",
       "22            0.000000                0.499953       0.479579    0.536435   \n",
       "3             0.000000                0.566657       0.479213    0.497455   \n",
       "1             0.549991                0.538450       0.479053    0.499024   \n",
       "10            0.583326                0.499988       0.479191    0.499895   \n",
       "2             0.583329                0.532252       0.479184    0.502323   \n",
       "8             0.499924                0.547458       0.479706    0.000000   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "59        0.000000           0.550953   0.528401            0.0   \n",
       "22        0.551913           0.499962   0.000000            0.0   \n",
       "3         0.085084           0.535704   0.495267            0.0   \n",
       "1         0.490756           0.511323   0.499951            0.0   \n",
       "10        0.498826           0.499974   0.499806            0.0   \n",
       "2         0.501172           0.518486   0.499993            0.0   \n",
       "8         0.000000           0.590833   0.000000            0.0   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "59         0.369063                0.000000       0.000000  0.0   \n",
       "22         0.350368                0.499959       0.000000  0.0   \n",
       "3          0.477563                0.000000       0.000000  0.0   \n",
       "1          0.498328                0.566660       0.000000  0.0   \n",
       "10         0.341959                0.499991       0.499983  0.0   \n",
       "2          0.503717                0.499996       0.000000  0.0   \n",
       "8          0.337198                0.599932       0.499881  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                 drive_csv  failure  \\\n",
       "59                0.520629           ST4000DM000.csv     3188   \n",
       "22                0.000000          WDC WD30EFRX.csv      171   \n",
       "3                 0.547612           ST8000DM002.csv      187   \n",
       "1                 0.566660          ST8000NM0055.csv      140   \n",
       "10                0.499991         ST12000NM0007.csv      106   \n",
       "2                 0.555552  HGST HMS5C4040BLE640.csv      171   \n",
       "8                 0.599932           ST4000DX000.csv       81   \n",
       "\n",
       "                                        file     lines        ok       size  \\\n",
       "59           drives_minified\\ST4000DM000.csv  40673227  40670038  635068446   \n",
       "22          drives_minified\\WDC WD30EFRX.csv   1265072   1264900   22320579   \n",
       "3            drives_minified\\ST8000DM002.csv   6387927   6387739  698445353   \n",
       "1           drives_minified\\ST8000NM0055.csv   5268760   5268619  569524555   \n",
       "10         drives_minified\\ST12000NM0007.csv   3422011   3421904  339870636   \n",
       "2   drives_minified\\HGST HMS5C4040BLE640.csv  12169350  12169178  584846426   \n",
       "8            drives_minified\\ST4000DX000.csv    293561    293479    7415004   \n",
       "\n",
       "    test  train  validate                  winner  winner_f_beta  \n",
       "59   319   2550       319       LightGBMAlgorithm       0.550953  \n",
       "22    17    136        17          KerasAlgorithm       0.551913  \n",
       "3     19    149        19  DecisionTreeClassifier       0.566657  \n",
       "1     14    112        14  RandomForestClassifier       0.566660  \n",
       "10    11     84        11      AdaBoostClassifier       0.583326  \n",
       "2     17    136        17      AdaBoostClassifier       0.583329  \n",
       "8      8     64         8  RandomForestClassifier       0.599932  "
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.55, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going on in the next Range upto 0.6, we find the first time the `LightGBM` lib by Microsoft that I also used in this project to compare it. Its also interisting that this algorithm could finish without waiting for hours to complete, as `ST4000DM000` was the second biggest dataset. It also have the higest amount of failures in total. A lot of algorithms did not finish on my local environment with this drive. I not used cloud providers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta from 0.6 to 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.499970</td>\n",
       "      <td>0.611081</td>\n",
       "      <td>0.479456</td>\n",
       "      <td>0.503649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.590870</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.356029</td>\n",
       "      <td>0.499970</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49997</td>\n",
       "      <td>WDC WD60EFRX.csv</td>\n",
       "      <td>65</td>\n",
       "      <td>drives_minified\\WDC WD60EFRX.csv</td>\n",
       "      <td>579285</td>\n",
       "      <td>579219</td>\n",
       "      <td>27132448</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>0.611081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.624981</td>\n",
       "      <td>0.583301</td>\n",
       "      <td>0.479268</td>\n",
       "      <td>0.502244</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499942</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.262093</td>\n",
       "      <td>0.499974</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>ST500LM012 HN.csv</td>\n",
       "      <td>67</td>\n",
       "      <td>drives_minified\\ST500LM012 HN.csv</td>\n",
       "      <td>775700</td>\n",
       "      <td>775632</td>\n",
       "      <td>52796831</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.624981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.624993</td>\n",
       "      <td>0.538453</td>\n",
       "      <td>0.479315</td>\n",
       "      <td>0.504851</td>\n",
       "      <td>0.499992</td>\n",
       "      <td>0.499983</td>\n",
       "      <td>0.496783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.505038</td>\n",
       "      <td>0.562495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Hitachi HDS5C3030ALA630.csv</td>\n",
       "      <td>150</td>\n",
       "      <td>drives_minified\\Hitachi HDS5C3030ALA630.csv</td>\n",
       "      <td>6641560</td>\n",
       "      <td>6641409</td>\n",
       "      <td>120329996</td>\n",
       "      <td>15</td>\n",
       "      <td>120</td>\n",
       "      <td>15</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.624993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.624993</td>\n",
       "      <td>0.499981</td>\n",
       "      <td>0.479335</td>\n",
       "      <td>0.496313</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.526988</td>\n",
       "      <td>0.496337</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.336183</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>ST6000DX000.csv</td>\n",
       "      <td>65</td>\n",
       "      <td>drives_minified\\ST6000DX000.csv</td>\n",
       "      <td>2220646</td>\n",
       "      <td>2220580</td>\n",
       "      <td>275485803</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>AdaBoostClassifier</td>\n",
       "      <td>0.624993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583328</td>\n",
       "      <td>0.499994</td>\n",
       "      <td>0.479297</td>\n",
       "      <td>0.499936</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.499974</td>\n",
       "      <td>0.499985</td>\n",
       "      <td>0.633327</td>\n",
       "      <td>0.499841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Hitachi HDS5C4040ALE630.csv</td>\n",
       "      <td>88</td>\n",
       "      <td>drives_minified\\Hitachi HDS5C4040ALE630.csv</td>\n",
       "      <td>4397831</td>\n",
       "      <td>4397742</td>\n",
       "      <td>78458395</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>9</td>\n",
       "      <td>MLPClassifier</td>\n",
       "      <td>0.633327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.478857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.640095</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010846</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>WDC WD30EZRS.csv</td>\n",
       "      <td>3</td>\n",
       "      <td>drives_minified\\WDC WD30EZRS.csv</td>\n",
       "      <td>4425</td>\n",
       "      <td>4421</td>\n",
       "      <td>79185</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>KerasAlgorithm</td>\n",
       "      <td>0.640095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.472941</td>\n",
       "      <td>0.486239</td>\n",
       "      <td>0.662142</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.372549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>WDC WD1600BPVT.csv</td>\n",
       "      <td>4</td>\n",
       "      <td>drives_minified\\WDC WD1600BPVT.csv</td>\n",
       "      <td>2495</td>\n",
       "      <td>2490</td>\n",
       "      <td>140770</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>KerasAlgorithm</td>\n",
       "      <td>0.662142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.479370</td>\n",
       "      <td>0.508575</td>\n",
       "      <td>0.096219</td>\n",
       "      <td>0.676451</td>\n",
       "      <td>0.502955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.502356</td>\n",
       "      <td>0.499982</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>Hitachi HDS723030ALA640.csv</td>\n",
       "      <td>73</td>\n",
       "      <td>drives_minified\\Hitachi HDS723030ALA640.csv</td>\n",
       "      <td>1429667</td>\n",
       "      <td>1429593</td>\n",
       "      <td>26526461</td>\n",
       "      <td>7</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>LightGBMAlgorithm</td>\n",
       "      <td>0.676451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "11            0.499970                0.611081       0.479456    0.503649   \n",
       "13            0.624981                0.583301       0.479268    0.502244   \n",
       "36            0.624993                0.538453       0.479315    0.504851   \n",
       "5             0.624993                0.499981       0.479335    0.496313   \n",
       "4             0.583328                0.499994       0.479297    0.499936   \n",
       "58            0.000000                0.000000       0.478857    0.000000   \n",
       "32            0.000000                0.000000       0.472941    0.486239   \n",
       "37            0.000000                0.499975       0.479370    0.508575   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "11        0.000000           0.590870   0.000000       0.000000   \n",
       "13        0.000000           0.499942   0.000000       0.000000   \n",
       "36        0.499992           0.499983   0.496783       0.000000   \n",
       "5         0.000036           0.526988   0.496337       0.000000   \n",
       "4         0.000020           0.499974   0.499985       0.633327   \n",
       "58        0.640095           0.000000   0.000000       0.000000   \n",
       "32        0.662142           0.000000   0.000000       0.000000   \n",
       "37        0.096219           0.676451   0.502955       0.000000   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "11         0.356029                0.499970            0.0  0.0   \n",
       "13         0.262093                0.499974            0.0  0.0   \n",
       "36         0.505038                0.562495            0.0  0.0   \n",
       "5          0.336183                0.000000            0.0  0.0   \n",
       "4          0.499841                0.000000            0.0  0.0   \n",
       "58         0.010846                0.000000            0.0  0.0   \n",
       "32         0.372549                0.000000            0.0  0.0   \n",
       "37         0.502356                0.499982            0.0  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm                    drive_csv  failure  \\\n",
       "11                 0.49997             WDC WD60EFRX.csv       65   \n",
       "13                 0.00000            ST500LM012 HN.csv       67   \n",
       "36                 0.00000  Hitachi HDS5C3030ALA630.csv      150   \n",
       "5                  0.00000              ST6000DX000.csv       65   \n",
       "4                  0.00000  Hitachi HDS5C4040ALE630.csv       88   \n",
       "58                 0.00000             WDC WD30EZRS.csv        3   \n",
       "32                 0.00000           WDC WD1600BPVT.csv        4   \n",
       "37                 0.00000  Hitachi HDS723030ALA640.csv       73   \n",
       "\n",
       "                                           file    lines       ok       size  \\\n",
       "11             drives_minified\\WDC WD60EFRX.csv   579285   579219   27132448   \n",
       "13            drives_minified\\ST500LM012 HN.csv   775700   775632   52796831   \n",
       "36  drives_minified\\Hitachi HDS5C3030ALA630.csv  6641560  6641409  120329996   \n",
       "5               drives_minified\\ST6000DX000.csv  2220646  2220580  275485803   \n",
       "4   drives_minified\\Hitachi HDS5C4040ALE630.csv  4397831  4397742   78458395   \n",
       "58             drives_minified\\WDC WD30EZRS.csv     4425     4421      79185   \n",
       "32           drives_minified\\WDC WD1600BPVT.csv     2495     2490     140770   \n",
       "37  drives_minified\\Hitachi HDS723030ALA640.csv  1429667  1429593   26526461   \n",
       "\n",
       "    test  train  validate                  winner  winner_f_beta  \n",
       "11     6     52         6  DecisionTreeClassifier       0.611081  \n",
       "13     7     53         7      AdaBoostClassifier       0.624981  \n",
       "36    15    120        15      AdaBoostClassifier       0.624993  \n",
       "5      6     52         6      AdaBoostClassifier       0.624993  \n",
       "4      9     70         9           MLPClassifier       0.633327  \n",
       "58     1      1         1          KerasAlgorithm       0.640095  \n",
       "32     1      2         1          KerasAlgorithm       0.662142  \n",
       "37     7     58         7       LightGBMAlgorithm       0.676451  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.6, 0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For to range upto 0.7, we first time find the \"default\" MLPClassifier. Suprisingly `Keras` did a really bad job on this drive. Its possible that my network that I created just not reflect the best Keras network at all..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FBeta to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <th>DecisionTreeClassifier</th>\n",
       "      <th>FakeAlgorithm</th>\n",
       "      <th>GaussianNB</th>\n",
       "      <th>KerasAlgorithm</th>\n",
       "      <th>LightGBMAlgorithm</th>\n",
       "      <th>LinearSVC</th>\n",
       "      <th>MLPClassifier</th>\n",
       "      <th>NearestCentroid</th>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <th>SGDClassifier</th>\n",
       "      <th>SVC</th>\n",
       "      <th>XGBoostGbtreeAlgorithm</th>\n",
       "      <th>drive_csv</th>\n",
       "      <th>failure</th>\n",
       "      <th>file</th>\n",
       "      <th>lines</th>\n",
       "      <th>ok</th>\n",
       "      <th>size</th>\n",
       "      <th>test</th>\n",
       "      <th>train</th>\n",
       "      <th>validate</th>\n",
       "      <th>winner</th>\n",
       "      <th>winner_f_beta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.49992</td>\n",
       "      <td>0.479530</td>\n",
       "      <td>0.722123</td>\n",
       "      <td>0.499920</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.499463</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579582</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD30EZRX.csv</td>\n",
       "      <td>25</td>\n",
       "      <td>drives_minified\\WDC WD30EZRX.csv</td>\n",
       "      <td>123578</td>\n",
       "      <td>123552</td>\n",
       "      <td>2225624</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>GaussianNB</td>\n",
       "      <td>0.722123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.480872</td>\n",
       "      <td>0.499655</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.436570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WDC WD800AAJS.csv</td>\n",
       "      <td>15</td>\n",
       "      <td>drives_minified\\WDC WD800AAJS.csv</td>\n",
       "      <td>14704</td>\n",
       "      <td>14688</td>\n",
       "      <td>715873</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>DecisionTreeClassifier</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AdaBoostClassifier  DecisionTreeClassifier  FakeAlgorithm  GaussianNB  \\\n",
       "56                 0.0                 0.49992       0.479530    0.722123   \n",
       "41                 0.0                 1.00000       0.480872    0.499655   \n",
       "\n",
       "    KerasAlgorithm  LightGBMAlgorithm  LinearSVC  MLPClassifier  \\\n",
       "56        0.499920                0.0   0.499463            0.0   \n",
       "41        0.000688                0.0   0.000000            0.0   \n",
       "\n",
       "    NearestCentroid  RandomForestClassifier  SGDClassifier  SVC  \\\n",
       "56         0.579582                     0.0            0.0  0.0   \n",
       "41         0.436570                     0.0            0.0  0.0   \n",
       "\n",
       "    XGBoostGbtreeAlgorithm          drive_csv  failure  \\\n",
       "56                     0.0   WDC WD30EZRX.csv       25   \n",
       "41                     0.0  WDC WD800AAJS.csv       15   \n",
       "\n",
       "                                 file   lines      ok     size  test  train  \\\n",
       "56   drives_minified\\WDC WD30EZRX.csv  123578  123552  2225624     2     20   \n",
       "41  drives_minified\\WDC WD800AAJS.csv   14704   14688   715873     1     12   \n",
       "\n",
       "    validate                  winner  winner_f_beta  \n",
       "56         2              GaussianNB       0.722123  \n",
       "41         1  DecisionTreeClassifier       1.000000  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_range(0.7, 1.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finlly the last range. We also have a fbeta Score of 1 for the drive `WDC WD800AAJS` - so we **can** predict correctly the failure of this model regarding my research!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "## Free-Form Visualization\n",
    "A visualization has been provided that emphasizes an important quality about the project with thorough discussion. Visual cues are clearly defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "winners = []\n",
    "algorithms = ['AdaBoostClassifier', 'DecisionTreeClassifier', 'FakeAlgorithm',\n",
    "       'GaussianNB', 'KerasAlgorithm', 'LightGBMAlgorithm', 'LinearSVC',\n",
    "       'MLPClassifier', 'NearestCentroid', 'RandomForestClassifier',\n",
    "       'SGDClassifier', 'SVC', 'XGBoostGbtreeAlgorithm']\n",
    "\n",
    "for algo in algorithms:\n",
    "    total = 0\n",
    "    for _i, row in merged_result.iterrows():\n",
    "        if row['winner'] == algo:\n",
    "            total += 1\n",
    "    winners.append(total)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "data": [
        {
         "hoverinfo": "label+percent",
         "labels": [
          "AdaBoostClassifier",
          "DecisionTreeClassifier",
          "FakeAlgorithm",
          "GaussianNB",
          "KerasAlgorithm",
          "LightGBMAlgorithm",
          "LinearSVC",
          "MLPClassifier",
          "NearestCentroid",
          "RandomForestClassifier",
          "SGDClassifier",
          "SVC",
          "XGBoostGbtreeAlgorithm"
         ],
         "marker": {
          "line": {
           "color": "#000000",
           "width": 2
          }
         },
         "textfont": {
          "size": 20
         },
         "textinfo": "value",
         "type": "pie",
         "uid": "e01e5512-a636-11e8-a051-fcaa14710b08",
         "values": [
          12,
          12,
          9,
          10,
          4,
          2,
          3,
          1,
          1,
          4,
          1,
          0,
          1
         ]
        }
       ],
       "layout": {
        "title": "Total Algorithm winners"
       }
      },
      "text/html": [
       "<div id=\"8d11445c-d363-4bd1-86c6-d0297b7eb538\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8d11445c-d363-4bd1-86c6-d0297b7eb538\", [{\"hoverinfo\": \"label+percent\", \"labels\": [\"AdaBoostClassifier\", \"DecisionTreeClassifier\", \"FakeAlgorithm\", \"GaussianNB\", \"KerasAlgorithm\", \"LightGBMAlgorithm\", \"LinearSVC\", \"MLPClassifier\", \"NearestCentroid\", \"RandomForestClassifier\", \"SGDClassifier\", \"SVC\", \"XGBoostGbtreeAlgorithm\"], \"marker\": {\"line\": {\"color\": \"#000000\", \"width\": 2}}, \"textfont\": {\"size\": 20}, \"textinfo\": \"value\", \"values\": [12, 12, 9, 10, 4, 2, 3, 1, 1, 4, 1, 0, 1], \"type\": \"pie\", \"uid\": \"e021fe70-a636-11e8-9f2f-fcaa14710b08\"}], {\"title\": \"Total Algorithm winners\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8d11445c-d363-4bd1-86c6-d0297b7eb538\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";Plotly.newPlot(\"8d11445c-d363-4bd1-86c6-d0297b7eb538\", [{\"hoverinfo\": \"label+percent\", \"labels\": [\"AdaBoostClassifier\", \"DecisionTreeClassifier\", \"FakeAlgorithm\", \"GaussianNB\", \"KerasAlgorithm\", \"LightGBMAlgorithm\", \"LinearSVC\", \"MLPClassifier\", \"NearestCentroid\", \"RandomForestClassifier\", \"SGDClassifier\", \"SVC\", \"XGBoostGbtreeAlgorithm\"], \"marker\": {\"line\": {\"color\": \"#000000\", \"width\": 2}}, \"textfont\": {\"size\": 20}, \"textinfo\": \"value\", \"values\": [12, 12, 9, 10, 4, 2, 3, 1, 1, 4, 1, 0, 1], \"type\": \"pie\", \"uid\": \"e021fe70-a636-11e8-9f2f-fcaa14710b08\"}], {\"title\": \"Total Algorithm winners\"}, {\"showLink\": true, \"linkText\": \"Export to plot.ly\"})});</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# see: https://plot.ly/python/pie-charts/\n",
    "\n",
    "trace = go.Pie(labels=algorithms, values=winners,\n",
    "               hoverinfo='label+percent', textinfo='value', \n",
    "               textfont=dict(size=20),\n",
    "               marker=dict(line=dict(color='#000000', width=2)))\n",
    "plotly.offline.iplot({\n",
    "    'data': [trace],\n",
    "    'layout': go.Layout(title=\"Total Algorithm winners\")\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AdaBoostClassifier` and `DecisionTreeClassifier` both share the first place in this competition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding this project, they where a lot of trouble. First transferring the data from the mixed form into a standard form that every possible S.M.A.R.T. value is present. Then merging this different \"raw\" files together. Then splitting each model type out of mixes and splitting again for test, train and validation set. After doing a lot of the work, I got a lot more out of memory errors than now. I had to add the drive_minify method, to reduce the total amount of data I pass to the algorithm. Before this refactoring, I loaded the entire csv file into RAM and than dropped the unneded columns - but before I could drop them, the RAM was already full and I could not continue. Calculating the size of test train and validation set was also at some point wrong, as calculating $3 \\cdot 0.1 = 0.3 -> 0 %$. But with no failures in test and validation set, the algorithms throw errors that a calculation is not possible. Having a very low amount of failures,I split the data in \"bigger\" parts to have more drives to run algorithms against.\n",
    "\n",
    "I also struggeled with not running the algorithms in sequence, as the IO operations by pandas `read_csv` not even scratch the performance of the installed SSD (A Samsung SSD 850 EVO 1 TB). So I tried the `Parallel` class provided by `joblib` and tried to use it as much as possible - but that made the debugging harder. Also for the huge drives, the data was to big to run it in parallel, so I had to do it in sequence for this case. - I learned a lot!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improvement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After working in total of 6 weeks on this capstone project, I skipped some algorithms that run for a very long time on some datasets. Having more cpu power and RAM allows to calculate more algorithms and generate more results. Also it is interisting to run them on even more frameworks like Apache Spark or pytorch to have a even bigger set of algorithms. Some cloud providers also offer their own implementation of different algorithms with hyperparameter tuning. This could also increase the number of algorithms that are comparing against each other.\n",
    "\n",
    "In general, this challange is taff, as the overall failure rate is only 1.8% of all drives in average. It is like searching a needle in a haystack (german: \"Nadel im Heuhaufen suchen\"). This is also reflected by the general low fbeta score for a lot of drives.\n",
    "\n",
    "I mixed drives with different capacity together. I could be interisting if splitting them by capacity somehow increases the accuracy. Also somebody could try to run the algorithms against all drives by a given manufacturer to have more failing drives in the datasets and compare them with my solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel like giving me feedback, write me on Twitter @dariusmurawski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
